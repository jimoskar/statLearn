---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 39"
author: "Alexander J Ohrt, Jim Totland"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

# Problem 1

## a)

We assume that ${\bf Y}$ is a multivariate normal, which gives the distribution  ${\bf Y} \sim N_{n}({\bf X} {\boldsymbol\beta},\sigma^2 {\bf I})$.

$$
\begin{split}
\mathsf{E}(\widetilde{\boldsymbol \beta}) &= \mathsf{E}((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf Y}) = (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathsf{E}({\bf Y}) \\ &= (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathsf{E}({\bf X}\boldsymbol{\beta} + \varepsilon) = (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
\end{split}
$$

and 

$$
\begin{split}
\mathsf{Cov}(\widetilde{\boldsymbol \beta}) &= \mathsf{Cov}((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf Y}) = ((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T)\mathsf{Cov}({\bf Y})((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T)^T \\ &= ((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T)\sigma^2 I({\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-T}) = \sigma^2((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-T}) \\
&= \sigma^2((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}),
\end{split}
$$

where we have used that $(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1} = (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-T}$ in the last equality (USIKKER PÅ DENNE SISTE! DET BLIR I HVERT FALL HELT RIKTIG Å BEHOLDE -T).
In both these equations it is apparent that the moments are equal to those of the OLS estimator when $\lambda = 0$. 

## b)

The requested moments of $\widetilde{f}(\mathbf{x}_0)$ are 

$$
\mathsf{E}(\widetilde{f}(\mathbf{x}_0)) = \mathsf{E}(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}) = \mathbf{x}_0^T\mathsf{E}(\widetilde{\boldsymbol \beta}) = \mathbf{x}_0^T (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
$$

and

$$
\begin{split}
\mathsf{Cov}(\widetilde{f}(\mathbf{x}_0)) &= \mathsf{Cov}(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}) 
= \mathbf{x}_0^T\mathsf{Cov}(\widetilde{\boldsymbol{\beta}})\mathbf{x}_0 \\ &= \sigma^2\mathbf{x}_0^T((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}) \mathbf{x}_0.
\end{split}
$$


## c)

The expected MSE at $\mathbf{x}_0$ is 

$$
\begin{split}
\mathsf{E}[(y_0 - \widetilde{f}(\mathbf{x}_0))^2] &= [\mathsf{E}(\widetilde{f}(\mathbf{x}_0)-f(\mathbf{x}_0)]^2+\mathsf{Var}(\widetilde{f}(\mathbf{x}_0) ) + \mathsf{Var}(\varepsilon) \\
&= [\mathsf{E}(\widetilde{f}(\mathbf{x}_0)) - \mathsf{E}(f(\mathbf{x}_0))]^2 + \mathsf{Cov}(\widetilde{f}(\mathbf{x}_0)) + \sigma^2I \\
&= [\mathbf{x}_0^T (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} - \mathbf{x}_0^T\boldsymbol{\beta}]^2 + \sigma^2\mathbf{x}_0^T((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}) \mathbf{x}_0 + \sigma^2 \\
\end{split}
$$
FOR NOE GRISERI

```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X = values$X
dim(X)
x0 = values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```

## d)

The expression $\widetilde{\boldsymbol{\beta}}(\lambda) = ((1+\lambda)I)^{-1}\hat{\boldsymbol{\beta}}$ (\textcolor{red}{Where is this from?})is inserted into `value` below. 

```{r}
library(ggplot2)
bias = function(lambda,X,x0,beta)
{
  p = ncol(X)
  lambda*diag(p)
  #value = mean(t(x0) %*% solve((1+lambda)*diag(p)) %*% beta - t(x0) %*% beta )^2
  #value = mean(t(x0) %*% solve(diag(p) + lambda*t(X) %*% X) %*% beta - t(x0) %*% beta )^2
  #value = mean(t(x0) %*% solve(t(X) %*% X + lambda*diag(p)) %*% t(X) %*% X %*% beta - t(x0) %*%   #beta)^2
  # Tror dette skal være rett algebraisk, men synes resultatet er merkelig kanskje!
  # Kanskje oppgaven ovenfor burde fullføres først?
  #value = mean(t(x0) %*% (diag(p) + 1/lambda*t(X)%*%X)%*%beta - t(x0) %*% beta)^2
  
  # All of these give different plots!
  
  # Are we not just supposed to use the result from 1c) ? Like this:
  
  v1 <- t(x0) %*% solve(t(X) %*% X + lambda * diag(p)) %*% t(X) %*% X %*%         beta - t(x0) %*% beta
  v2 <- sigma^2 * t(x0) %*% (solve(t(X) %*% X + lambda * diag(p)) %*% t(X)         %*% X %*% t(solve(t(X) %*% X + lambda * diag(p)))) %*% x0
  v3 <- sigma^2 
  value <-  v1^2 + v2 + v3  
  return(value)
}
lambdas = seq(0, 2, length.out = 500)
BIAS = rep(NA,length(lambdas))
for (i in 1:length(lambdas)) BIAS[i] = bias(lambdas[i], X, x0, beta)
dfBias = data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) + 
  geom_line(color = "red")+
  xlab(expression(lambda))+
  ylab(expression(bias^2))
```

Comments: I think this is wrong. Perhaps a good idea to finish c) first, to get a better expression for the bias also. I think the expression is correct however, despite it not being "forkortet", but this should be done later. However, I think the result is weird. 
 
### e) 

```{r}
variance = function(lambda, X, x0, sigma)
{
  p = ncol(X)
  inv = solve(t(X)%*%X+lambda*diag(p))
  value = sigma*(inv%*%t(X)%*%X%*%inv)
  return(value)
}
lambdas = seq(0, 2, length.out = 500)
VAR=rep(NA,length(lambdas))
for (i in 1:length(lambdas)) VAR[i]=variance(lambdas[i], X, x0, sigma)
dfVar = data.frame(lambdas = lambdas, var = VAR)
ggplot(dfVar, aes(x = lambdas, y = var))+
  geom_line(color = "green4")+
  xlab(expression(lambda))+
  ylab("variance")
```

Comments: The variance decreases with lambda. It begins very high (which I do not know if holds with the regular estimator when $\lambda = 0$).

## f)

```{r}
exp_mse = BIAS + VAR + sigma
lambdas[which.min(exp_mse)]
```

```{r}
dfAll = data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)
ggplot(dfAll)+
  geom_line(aes(x = lambda, y = exp_mse), color = "blue")+
  geom_line(aes(x = lambda, y = bias), color = "red")+
  geom_line(aes(x = lambda, y = var), color = "green4")+
    xlab(expression(lambda))+
    ylab(expression(E(MSE)))
```

Comments: Again, unsure if this is correct. 

# Problem 2

```{r}
# read file
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```

## a) 

Inspection of the data. Assuming that 0 = False and 1 = True (as usual), which means that a person has not deceased when `deceased` = 0 and vice versa.

```{r}
knitr::kable(table(Deceased = d.corona$deceased)) # Prøvde å sette kolonne-navn, men fikk det ikke til.

knitr::kable(table(d.corona$country, d.corona$sex))

knitr::kable(table(d.corona$deceased, d.corona$sex))

knitr::kable(table(d.corona$country, d.corona$deceased)) # Må hente ut øverste raden herfra, men klarte det ikke. 
```

## b) 

```{r}
# Just in case they are not factors (this should perhaps be deleted later, since we could have checked if they were factors earlier).
d.corona$sex = factor(d.corona$sex)
d.corona$country = factor(d.corona$country)
lm.fit <- lm(deceased ~ ., data = d.corona) # perhaps a linear model is not the correct model to use?
summary(lm.fit)
```

(i) The probability to die of covid for a male age 75 living in Korea can be predicted from the model. The prediction is ...
(ii) The p-value for `sexmale` is relatively small, but we would not say that there is clear evidence that males have higher probability to die than females. The p-value could be low by chance also, and since it is not amazingly small, we do not think it is appropriate evidence of the question.
(iii) The p-value of `countryjapan` is the smallest of the country-coefficients, which shows that it could potentially be important as a predictor. In the least, it does not exclude this possibility.
(iv) Quantify the odds 

## c) 

(i) 

(ii)

I am guessing that "the appropriate model" would be to use linear regression, as used earlier, since this is the only regression-model we have learned thus far. Not quite sure if they want us to leave out some of the predictors however, or if they just want us to keep all of them in the model. 

## d)

I will make the models before answering the questions, which can/will be deleted later (since these are multiple choice).

```{r}
# Make data sets.
trainID = sample(x = 1:nrow(d.corona), size = 1500, replace = F) # E.g
train = d.corona[trainID,]
test = d.corona[!trainID,]
nrow(d.corona)

# Ikke så greit å bruke indekser i d.corona tydeligvis. Kan se mer på dette senere dersom det virker 
# som en fornuftig måte å løse oppgaven på...

library(MASS)
lda.fit <- lda(deceased ~ ., data = train)
lda.fit.pred <- predict(lda.fit, newdata = test)$class
table(predicted = lda.fit.pred, true = test$deceased)

# This did not work. 

```

EVENTUALLY THERE WILL ONLY BE TRUE OR FALSE IN EACH OF THESE, BUT I HAVE WRITTEN MY THOUGHTS HERE FIRST. 

(i) `r length(d.corona$deceased[d.corona$deceased == T])/length(d.corona$deceased[d.corona$deceased == F]) * 100` % seems to be the percentage of deaths in the dataset. This is not equal to the percentage given in the question (by accident). If they were equal I would say that this statement is TRUE. because, as I have understood it, the "null rate" for misclassification can be obtained in this case by always classifying the individual as healthy. 

(ii) Following the argumentation in the above point, LDA is useless if the misclassification rate is higher than approx 5 %. Need to make a LDA in code to test?

(iii) Can we answer this from theory?

(iv) The same concern goes for this statement as for the ones above. 

# Problem 3

```{r, eval=T}
#read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO" # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```

## a)

```{r}
logReg = glm(diabetes~., data = train, family = "binomial")
summary(logReg)
```

(i) (assuming they want us to do it theoretically, and not in R, e.g. by plotting) We have that 

$$
\begin{split}
p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}} &= \frac{e^{\eta_i(x)}}{1+e^{\eta_i(x)}} \implies \frac{p_i}{1-p_i} = \frac{\frac{e^{\eta_i(x)}}{1+e^{\eta_i(x9)}}}{1-\frac{e^{\eta_i(x)}}{1+e^{\eta_i(x)}}} = e^{\eta_i(x)} \\ \implies  \mathsf{log}\left(\frac{p_i}{1-p_i}\right) &= \eta_i(x) = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}
\end{split}
$$

(ii) Can be seen below

```{r}
glm.probs <- predict(logReg, newdata = test, type = "response")
glm.preds <- ifelse(glm.probs > 0.5, 1, 0) #"Present", "Non-present") # Could also use these names. 
conf.table.glm <- table(predicted = glm.preds, true = test$diabetes)
conf.table.glm
```
The sensitivity is `r conf.table.glm[2,2]/(conf.table.glm[2,2]+conf.table.glm[1,2])` and the specificity is `r conf.table.glm[1,1]/(conf.table.glm[1,1]+conf.table.glm[2,1])`. (perhaps show the calculations more clearly in another way later.)

## b)

(i) (are we supposed to give estimates in this case?: "Explain what they are in the diabetes classification problem") $\pi_k$ is the prior probability, given by $\pi_k = P(y=k)$. $\boldsymbol{\mu}_k$ is the mean vector of class $k$, when we have assumed that each $f_k(\mathbf{x})$ is normal. In this case, the class $1$, i.e. presence of diabetes, has the mean vector $\boldsymbol{\mu}_1$ and the class $0$, i.e. non-presence of diabetes, has the mean vector $\boldsymbol{\mu}_0$. $\boldsymbol{\Sigma}$ is the covariance matrix of each class, when assumed that the distribution of each class are normal. In LDA $\boldsymbol{\Sigma}_k = \boldsymbol{\Sigma} \quad \forall k$, where as in QDA each $\boldsymbol{\Sigma}_k$ are allowed to be different. $f_k(x)$ is the density function of $X$ for an observation that comes from class $k$. These are assumed to be normal in LDA and QDA. 

(ii) The fits are seen below

```{r}
lda.diabetes <- lda(diabetes~., data = train)
qda.diabetes <- qda(diabetes~., data = train)

lda.diabetes.probs <- predict(lda.diabetes, newdata = test)$posterior
lda.preds <- ifelse(lda.diabetes.probs > 0.5, 1, 0)
#conf.table.lda.diabetes <- table(predicted = lda.preds, true = test$diabetes)
#conf.table.lda.diabetes

# Why are the probs half the length of the test data? I cannot seem to find the reason! :(

# Could also use $class below, but not sure how to set the cut-off probability in that case. 
qda.diabetes.probs <- predict(qda.diabetes, newdata = test)$posterior
qda.preds <- ifelse(qda.diabetes.probs > 0.5, 1, 0)
#conf.table.qda.diabetes <- table(predicted = qda.preds, true = test$diabetes)
#conf.table.qda.diabetes
```

The difference between the methods is that the covariances are equal across all classes (in this case: both classes) in LDA, which gives linear discriminant functions and a linear decision boundary. On the contrary, the covariances in each class when using QDA are allowed to be different, which gives a quadratic discriminant function in each class and a quadratic decision boundary. 

## c)

(i) In the KNN approach, a new observation is classified by using the $K$ nearest points (in Euclidean norm) to the observation in question to estimate the probability of the new point belonging to each of the different classes in the response. If the estimated probability of the point belonging to a class is larger than a pre-selected threshold (usually 0.5), the point is classified as belonging to this class. (perhaps it is just classified to the class that has the largest estimated probability?)

(ii) I would probably use cross validation to test the predictive power for different $K$ (elaborate).

(iii) The KNN classification fit can be seen below

```{r}
library(class)
set.seed(123) # for reproducibility.
knn.diabetes <- knn(train = train, test = test, cl = train$diabetes, k=25, prob=T)
#table(predicted = knn.diabetes, true = train$diabetes) # Similar dimensionality problem as above!?!?

```

## d) ROC curves


# Problem 4

## a)



