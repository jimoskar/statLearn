---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 1: Group 39"
author: "Alexander J Ohrt, Jim Totland"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(MASS)
library(tidyverse)
library(class)
library(pROC)
#library(plotROC)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3, comment = "#>", fig.align = "center")
```

# Problem 1

## a)

By extending the given univariate regression problem to a multivariate regression problem that allows for several observations, we have that ${\bf Y} = {\bf X} {\boldsymbol\beta} + \varepsilon I$, where $I$ is the identity matrix of the correct dimensions. Hence, we have $\mathsf{E}({\bf Y}) =  \mathsf{E}({\bf X}\boldsymbol{\beta} + \varepsilon I) = \mathbf{X}\boldsymbol{\beta}$ and $\mathsf{Cov}({\bf Y}) = \sigma I$, without assuming anything else than that each of the $\varepsilon$'s are independent of each other. Then

<!-- Remove this for now, because it is not necessary to assume this at this stage -->
<!-- We assume that ${\bf Y}$ is a multivariate normal, which gives the distribution  ${\bf Y} \sim N_{n}({\bf X} {\boldsymbol\beta},\sigma^2 {\bf I})$. -->

$$
\begin{split}
\mathsf{E}(\widetilde{\boldsymbol \beta}) &= \mathsf{E}((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf Y}) = (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathsf{E}({\bf Y}) = (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
\end{split}
$$

and 

$$
\begin{split}
\mathsf{Cov}(\widetilde{\boldsymbol \beta}) &= \mathsf{Cov}((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf Y}) = ((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T)\mathsf{Cov}({\bf Y})((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T)^T \\ &= ((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T)\sigma^2 I{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-T} = \sigma^2((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-T}) \\
&= \sigma^2((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}),
\end{split}
$$

where we have used that $(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1} = (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-T}$ in the last equality.
In both these equations it is apparent that the moments are equal to those of the OLS estimator when $\lambda = 0$. 

## b)

The requested moments of $\widetilde{f}(\mathbf{x}_0) = \mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}$ are 

$$
\mathsf{E}(\widetilde{f}(\mathbf{x}_0)) = \mathsf{E}(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}) = \mathbf{x}_0^T\mathsf{E}(\widetilde{\boldsymbol \beta}) = \mathbf{x}_0^T (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
$$

and

$$
\begin{split}
\mathsf{Cov}(\widetilde{f}(\mathbf{x}_0)) &= \mathsf{Cov}(\mathbf{x}_0^T \widetilde{\boldsymbol{\beta}}) 
= \mathbf{x}_0^T\mathsf{Cov}(\widetilde{\boldsymbol{\beta}})\mathbf{x}_0 \\ &= \sigma^2\mathbf{x}_0^T((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}) \mathbf{x}_0.
\end{split}
$$


## c)

The expected MSE at $\mathbf{x}_0$ is 

$$
\begin{split}
\mathsf{E}[(y_0 - \widetilde{f}(\mathbf{x}_0))^2] &= [\mathsf{E}(\widetilde{f}(\mathbf{x}_0)-f(\mathbf{x}_0)]^2+\mathsf{Var}(\widetilde{f}(\mathbf{x}_0) ) + \mathsf{Var}(\varepsilon) \\
&= [\mathsf{E}(\widetilde{f}(\mathbf{x}_0)) - \mathsf{E}(f(\mathbf{x}_0))]^2 + \mathsf{Cov}(\widetilde{f}(\mathbf{x}_0)) + \sigma^2 \\
&= [\mathbf{x}_0^T (\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} - \mathbf{x}_0^T\boldsymbol{\beta}]^2 + \sigma^2\mathbf{x}_0^T((\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^T{\bf X}(\mathbf{X}^T\mathbf{X}+\lambda {\bf I})^{-1}) \mathbf{x}_0 + \sigma^2. \\
\end{split}
$$

Since there is no obvious way to simplify this, it will be left like this. This is also practical since it is easy to distinguish between the irreducible error, the variance of prediction and the squared bias. 

## d)

```{r, echo=TRUE, eval=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X = values$X
dim(X)
x0 = values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```

```{r}
bias = function(lambda,X,x0,beta)
{
  p = ncol(X)
  value <-  (t(x0) %*% solve(t(X) %*% X + lambda * diag(p)) %*% t(X) %*% X %*% beta - t(x0) %*% beta)^2
  return(value)
}
lambdas = seq(0, 2, length.out = 500)
BIAS = rep(NA,length(lambdas))
for (i in 1:length(lambdas)) BIAS[i] = bias(lambdas[i], X, x0, beta)
dfBias = data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) + 
  geom_line(color = "red")+
  
  xlab(expression(lambda))+
  ylab(expression(bias^2))
```
 
Comments: The graph shows that the bias of the ridge regression estimator increases as $\lambda$ grows. OLS is unbiased, so, as expected, the bias is zero when $\lambda = 0$. Note that $\lambda \approx 0.5$ appears to be a sweet spot for this estimator when comparing all $\lambda > 0$, since the bias is low there. Perhaps this can be useful later. 

### e) 

```{r}
variance = function(lambda, X, x0, sigma)
{
  p = ncol(X)
  inv = solve(t(X)%*%X+lambda*diag(p))
  value = sigma^2*t(x0) %*% (inv %*% t(X) %*% X %*% inv) %*% x0
  return(value)
}
lambdas = seq(0, 2, length.out = 500)
VAR=rep(NA,length(lambdas))
for (i in 1:length(lambdas)) VAR[i]=variance(lambdas[i], X, x0, sigma)
dfVar = data.frame(lambdas = lambdas, var = VAR)
ggplot(dfVar, aes(x = lambdas, y = var))+
  geom_line(color = "green4")+
  
  xlab(expression(lambda))+
  ylab("variance")
```

Comments: The variance begins at $\approx$ `r round(variance(0, X, x0, sigma), digits = 3)` and decreases with $\lambda$. Hence, it is apparent that the ridge regression estimator is advantageous, when looking at solely variance, compared to the OLS estimator, since the variance is decreasing with $\lambda$. Despite this, when adding the bias and the variance, the OLS estimator may still have a lower expected MSE than the ridge regression estimator. Finally, note that the changes in the variance are larger for $\lambda \in [0,2]$ than the changes in the bias (as seen in the plot in task d)), which indicates that the variance dominates the change in expected MSE for the ridge regression estimation. 

## f)

```{r}
exp_mse = BIAS + VAR + sigma^2
lambdas[which.min(exp_mse)]
```

```{r}
dfAll = data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)
ggplot(dfAll)+
  geom_line(aes(x = lambda, y = exp_mse), color = "blue")+
  
  geom_line(aes(x = lambda, y = bias), color = "red")+
  geom_line(aes(x = lambda, y = var), color = "green4")+
    xlab(expression(lambda))+
    ylab(expression(E(MSE)))
```

Comments: Now we are able to conclude that the ridge regression estimator has a lower expected MSE compared to the OLS estimator, as gathered from the blue line in the plot. The optimal value of $\lambda$, which minimizes MSE, is $\approx$ `r round(lambdas[which.min(exp_mse)], digits = 3)`. Hence, despite the fact that the bias is higher for the ridge regression estimator, the total expected MSE is lower, since the decrease in variance counters the increase in bias. 

# Problem 2

```{r}
# read file
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```

## a) 

The tables are reported below. 

```{r}
# knitr::kable(table(d.corona$deceased)) # Hvorfor ta bort de som fungerer?

table1 <- summarise(d.corona, deceased = sum(d.corona$deceased),
                   non_deceased = nrow(d.corona) - deceased)
knitr::kable(table1)

table2 <- table(d.corona$country, d.corona$sex)
knitr::kable(table2)

table3 <- table(d.corona$deceased, d.corona$sex)
rownames(table3) = c("non-deceased", "deceased")
knitr::kable(table3)

d.france <- filter(d.corona, country == "France")
table4 <- table(d.france$deceased, d.france$sex)
rownames(table4) = c("non-deceased", "deceased")
knitr::kable(table4)
```

## b) 

```{r}
glm.fit <- glm(deceased ~ ., family = "binomial", data = d.corona)
summary(glm.fit)

```

(i) The probability to die of covid for a male age 75 living in Korea can be predicted from the model. The prediction is found by

```{r}
x0 = data.frame(sex = "male", age = 75, country = "Korea")
predict(glm.fit, newdata = x0, type = "response")
```


(ii) The p-value associated with `sexmale` is relatively small, and since the coefficient is positive, this constitutes evidence that males have a higher probability of dying.
ER DU UENIG??
(but we would not say that there is clear evidence that males have higher probability to die than females. The p-value could be low by chance also, and since it is not amazingly small, we do not think it is appropriate evidence of the question.)

(iii) Yes. Both the `countryjapan` and `countryKorea` coefficients have low p-values and are negative, which evidence that the probabilty of decease is lower in Japan and Korea compared to the reference category, France. `countryIndonesia` is not significant (high p-value), so there is no evidence that the probability of decease is any higher in Indonesia than in France.
HVA TENKER DU?

(iv) Since we have used logistic regression, the odds of decease, given an observation $\boldsymbol{x}$ is 

$$ \frac{p(\boldsymbol{x})}{1- p(\boldsymbol{x})} = e^{\boldsymbol{x}^T\hat{\boldsymbol{\beta}}}. $$
Thus, the odds of decease increases by a factor of $e^{10 \hat{\beta_{\tt{age}}}} \approx 1.31$ when `age` increases by 10 and all other covariates are held constant.

## c) 

```{r}
log.fit1 <- glm(deceased ~. + sex:age, data = d.corona, family="binomial")
summary(log.fit1)
```

(i) As the summary shows, the interaction effect between sexmale and age is not significant. Hence, although the coefficient is slightly positive, we cannot conclude that age is a greater risk factor for men.
(As seen above, the coefficient for male is positive (`r coefficients(log.fit1)[2]`), which means that a male has higher risk than a female. This is because the value of this coefficient is added on top of the interceipt, which corresponds to an individual that is in the reference level, i.e. a female.)

```{r}
log.fit2 <- glm(deceased ~ . + age:country, data = d.corona, family="binomial")
summary(log.fit2)
```

(ii) As seen above, we have fitted the full logistic regression with an interaction term between `age` and `country`. Since `France` is the reference level, and the interaction-term coefficient `age:countryindonesia` has the value `r coefficients(log.fit2)[7]` with a relatively low p-value, we can infer that age is a greater risk factor for the French population than for the Indonesian population. 

## d)

I will make the models before answering the questions, which can/will be deleted later (since these are multiple choice).

```{r}
# Make data sets.
trainID = sample(x = 1:nrow(d.corona), size = 1500, replace = F) # E.g
train = d.corona[trainID,]
test = d.corona[!trainID,]
nrow(d.corona)

# Ikke så greit å bruke indekser i d.corona tydeligvis. Kan se mer på dette senere dersom det virker 
# som en fornuftig måte å løse oppgaven på...

lda.fit <- lda(deceased ~ ., data = train)
lda.fit.pred <- predict(lda.fit, newdata = test)$class
table(predicted = lda.fit.pred, true = test$deceased)

# This did not work. 

```

EVENTUALLY THERE WILL ONLY BE TRUE OR FALSE IN EACH OF THESE, BUT I HAVE WRITTEN MY THOUGHTS HERE FIRST. 

(i) `r length(d.corona$deceased[d.corona$deceased == T])/(nrow(d.corona)) * 100` % \textcolor{green}{divided by the length of the whole dataset!} seems to be the percentage of deaths in the dataset. This is not equal to the percentage given in the question (by accident). If they were equal I would say that this statement is TRUE. because, as I have understood it, the "null rate" for misclassification can be obtained in this case by always classifying the individual as healthy. 

(ii) Following the argumentation in the above point, LDA is useless if the misclassification rate is higher than approx 5 %. Need to make a LDA in code to test?
\textcolor{green}{In this case, we are probably more interested in the probabilites than the classification, so LDA is not so relevant?}
\textcolor{red}{spørre om dette på mandan!}

(iii) Can we answer this from theory?

(iv) The same concern goes for this statement as for the ones above. 

# Problem 3

```{r, eval=T}
# Read file.
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO" # Google file ID.
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```

## a)

```{r}
logReg = glm(diabetes~., data = train, family = "binomial")
summary(logReg)
```

(i) We have that 

$$
\begin{split}
p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}} &:= \frac{e^{\eta_i(x)}}{1+e^{\eta_i(x)}} \implies \frac{p_i}{1-p_i} = \frac{\frac{e^{\eta_i(x)}}{1+e^{\eta_i(x)}}}{1-\frac{e^{\eta_i(x)}}{1+e^{\eta_i(x)}}} = e^{\eta_i(x)} \\ \implies  \mathsf{logit}(p_i) = \mathsf{log}\left(\frac{p_i}{1-p_i}\right) &= \eta_i(x) = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}
\end{split}
$$

(ii) Can be seen below

```{r}
glm.probs <- predict(logReg, newdata = test, type = "response")
glm.preds <- ifelse(glm.probs > 0.5, 1, 0) 
conf.table.glm <- table(predicted = glm.preds, true = test$diabetes)
conf.table.glm
```

The sensitivity is `r conf.table.glm[2,2]`/(`r conf.table.glm[2,2]` + `r conf.table.glm[1,2]`) $\approx$ `r  round(conf.table.glm[2,2]/(conf.table.glm[2,2]+conf.table.glm[1,2]), digits = 3)` and the specificity is `r conf.table.glm[1,1]`/(`r conf.table.glm[1,1]` + `r conf.table.glm[2,1]`) $\approx$ `r round(conf.table.glm[1,1]/(conf.table.glm[1,1]+conf.table.glm[2,1]), digits = 3)`.

## b)

(i) (are we supposed to give estimates in this case?: "Explain what they are in the diabetes classification problem") $\pi_k$ is the prior probability, given by $\pi_k = P(y=k)$. $\boldsymbol{\mu}_k$ is the mean vector of class $k$, when we have assumed that each $f_k(\mathbf{x})$ is normal. In this case, the class $1$, i.e. presence of diabetes, has the mean vector $\boldsymbol{\mu}_1$ and the class $0$, i.e. non-presence of diabetes, has the mean vector $\boldsymbol{\mu}_0$. $\boldsymbol{\Sigma}$ is the covariance matrix of each class, when assumed that the distribution of each class are normal. In LDA $\boldsymbol{\Sigma}_k = \boldsymbol{\Sigma} \quad \forall k$, where as in QDA each $\boldsymbol{\Sigma}_k$ are allowed to be different. $f_k(x)$ is the density function of $X$ for an observation that comes from class $k$. These are assumed to be normal in LDA and QDA. 

(ii) The fits are seen below

```{r}
lda.diabetes <- lda(diabetes~., data = train)
qda.diabetes <- qda(diabetes~., data = train)

lda.diabetes.probs <- predict(lda.diabetes, newdata = test)$posterior[, 2] # Only need the prob om diabetes present!
lda.preds <- ifelse(lda.diabetes.probs > 0.5, 1, 0)
conf.table.lda.diabetes <- table(predicted = lda.preds, true = test$diabetes)
conf.table.lda.diabetes

qda.diabetes.probs <- predict(qda.diabetes, newdata = test)$posterior[, 2]
qda.preds <- ifelse(qda.diabetes.probs > 0.5, 1, 0)
conf.table.qda.diabetes <- table(predicted = qda.preds, true = test$diabetes)
conf.table.qda.diabetes
```

The sensitivity and specificity for LDA are thus `r conf.table.lda.diabetes[2,2]/sum(conf.table.lda.diabetes[, 2])` and `r conf.table.lda.diabetes[1,1]/sum(conf.table.lda.diabetes[, 1])`, respectively. The sensitivity and specificity of QDA are `r conf.table.qda.diabetes[2,2]/sum(conf.table.qda.diabetes[, 2])` and `r conf.table.qda.diabetes[1,1]/sum(conf.table.qda.diabetes[, 1])`, respectively.

The difference between the methods is that the covariances are equal across all classes (in this case: both classes) in LDA, which gives linear discriminant functions and a linear decision boundary. On the contrary, the covariances in each class when using QDA are allowed to be different, which gives a quadratic discriminant function in each class and a quadratic decision boundary. 

## c)

(i) In the KNN approach, a new observation is classified by using the $k$ nearest points (in Euclidean norm) to the observation in question, to estimate the probability of the new point belonging to each of the different classes in the response. The new point is classified to the class with the highest estimated probability. 

(ii) I would probably use cross validation to test the predictive power for different $K$ (elaborate, saw it in the lecture on linear regression, but not quite sure how it is done in practice).

(iii) The KNN classification fit can be seen below

```{r}
set.seed(123) # For reproducibility.
knn.diabetes <- knn(train = train, test = test, cl = train$diabetes, k=25, prob=T)
conf.table.knn <- table(predicted = knn.diabetes, true = test$diabetes) 
conf.table.knn
```

The sensitivity is `r conf.table.knn[2,2]`/(`r conf.table.knn[2,2]` + `r conf.table.knn[1,2]`) $\approx$ `r  round(conf.table.knn[2,2]/(conf.table.knn[2,2]+conf.table.knn[1,2]), digits = 3)` and the specificity is `r conf.table.knn[1,1]`/(`r conf.table.knn[1,1]` + `r conf.table.knn[2,1]`) $\approx$ `r round(conf.table.knn[1,1]/(conf.table.knn[1,1]+conf.table.knn[2,1]), digits = 3)`.

## d) ROC curves

```{r}
knn.probs <- ifelse(knn.diabetes == 0, 1 - attributes(knn.diabetes)$prob, attributes(knn.diabetes)$prob)

ldaroc = roc(response = test$diabetes, predictor = lda.diabetes.probs, direction = "<")
qdaroc = roc(response = test$diabetes, predictor = qda.diabetes.probs, direction = "<")
knnroc = roc(response = test$diabetes, predictor = knn.probs, direction = "<")


ggroc(list(LDA  = ldaroc, QDA = qdaroc, KNN = knnroc))
```
 The area under the ROC curves is `r auc(ldaroc)` for LDA, `r auc(qdaroc)` for QDA and `r auc(knnroc)` for KNN. Based on this, LDA preforms the best. KNN is not very interpretable? logistic regression is most intepretable, but that is not included???

# Problem 4

## a)

$$
\begin{split}
\hat y_{(-i)} &= \mathbf{x}_i^T \hat{\boldsymbol{\beta}}_{(-i)} \\
&= \mathbf{x}_i^T (X_{(-i)}^T X_{(-i)})^{-1}X_{(-i)}^T \mathbf{y}_{(-i)} \\
&= \mathbf{x}_i^T (X^TX - \mathbf{x}_i\mathbf{x}_i^T)^{-1}(X^T \mathbf{y}-\mathbf{x}_i y_i) \\
&= \mathbf{x}_i^T\left[(X^TX)^{-1} + \frac{(X^TX)^{-1}\mathbf{x}_i\mathbf{x}_i^T(X^TX)^{-1}}{1-\mathbf{x}_i^T(X^TX)^{-1}\mathbf{x}_i}\right](X^T\mathbf{y}-\mathbf{x}_i y_i), \quad \text{Sherman-Morrison} \\
&= \mathbf{x}_i^T\left[(X^TX)^{-1} + \frac{(X^TX)^{-1}\mathbf{x}_i\mathbf{x}_i^T(X^TX)^{-1}}{1-h_i}\right](X^T\mathbf{y}-\mathbf{x}_i y_i) \\
&= \mathbf{x}_i^T(X^TX)^{-1}X^T\mathbf{y} - \mathbf{x}_i^T(X^TX)^{-1}\mathbf{x}_i y_i+\mathbf{x}_i^T\frac{(X^TX)^{-1}\mathbf{x}_i\mathbf{x}_i^T(X^TX)^{-1}}{1-h_i}X^T\mathbf{y}-\mathbf{x}_i^T\frac{(X^TX)^{-1}\mathbf{x}_i\mathbf{x}_i^T(X^TX)^{-1}}{1-h_i}\mathbf{x}_i y_i \\
&= \mathbf{x}_i^T\hat{\boldsymbol{\beta}} - \mathbf{x}_i^T\hat{\beta}_i + \frac{1}{1-h_i}h_i\mathbf{x}_i^T\hat{\boldsymbol{\beta}} - \frac{1}{1-h_i}h_i\mathbf{x}_i^T\hat{\beta}_i \quad \text{Usikker på notasjon i 2. og 4. ledd}\\
&= 
\end{split}
$$


## b)

(i) FALSE
(ii) TRUE
(iii) ?log-transform of the response??? (mener de logistic regression her?) (eller mener de transform av predictors?)
Litt usikker på hva de mener. Formelen vil jo gjelde hvis for en $\tilde{y_i} = log(y_i)$, men 
da gjelder den jo ikke for $y_i$.
(iv) FALSE (since the model is only fitted and trained on one combination of the two folds in the validation set approach, and not "the other way around" also, which would be the case in 2-fold cross validation.)

# Problem 5

```{r}
id <- "19auu8YlUJJJUsZY8JZfsCTWzDm6doE7C" # google file ID
d.bodyfat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```

## a)

```{r}
lm.fit5 <- lm(bodyfat ~ age + weight + bmi, data = d.bodyfat)
summary(lm.fit5)
```

The $R^2$ is $\approx$ `r round(summary(lm.fit5)$r.squared, digits = 4)`.

## b)

DO THEY WANT US TO DO IT MANUALLY OR TO USE THE BOOTSTRAP-FUNCTIONS IN R?

```{r}
set.seed(4268)

# Not sure how to tell it to calculated the r.squared statistic.
# library(boot)
# boot(d.bodyfat, r.squared, R=1000)

# Doing it manually instead
B <- 1000
n <- 100 #e.g
r.squared <- rep(NA, B)
for (b in 1:B){
  dat <- d.bodyfat[sample(x = nrow(d.bodyfat), size = n, replace = T), ]
  fit <- lm(bodyfat ~ age + weight + bmi, data = dat)
  r.squared[b] <- summary(fit)$r.squared
}

ggplot(data=data.frame(x=r.squared),aes(x=x))+
  geom_density()

# Derive the standard error and the 95% confidence interval. 
# Interpret. 
```

