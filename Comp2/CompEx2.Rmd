---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 2: Group 39"
author: "Alexander J. Ohrt, Jim Totland"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---


```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
library(tidyverse)
library(ggplot2)
library(ggfortify)
library(leaps)
library(glmnet)
library(tree)
library(caret)
library(randomForest)
library(readr)
library(e1071)
library(dplyr)
library(gbm)
library(MASS)

knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=5, fig.height=5, comment = "#>", fig.align = "center")
```

# Problem 1
## a)
FALSE, TRUE, TRUE, FALSE

\textcolor{red}{Ikke sikker på om den første er så opplagt, siden krymping elle fjerning av kovariater burde øke bias, noe som gjør at jeg tenker at MSE for treningsdataen generelt godt kan øke. De tre andre er jeg enig i :)}
\textcolor{green}{Se første svar her :)  https://stats.stackexchange.com/questions/347730/why-ridge-regression-increase-and-not-decrease-the-models-error}

## b)
First we make plots for the $R_\mathrm{adj}^2$, $BIC$, $C_p$ and cross validated prediction error. The red dots show the best amount of variables used in the subset, according to each of the different model choice criteria. 

```{r}
id <- "1iI6YaqgG0QJW5onZ_GTBsCvpKPExF30G"  # google file ID
catdat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), 
                   header = T)

set.seed(4268)
train.ind = sample(1:nrow(catdat), 0.5 * nrow(catdat))
catdat.train = catdat[train.ind, ]
catdat.test = catdat[-train.ind, ]


# Perform best subset selection using all the predictors and the training data
n <-  ncol(catdat.train) - 1 # Number of predictors
bss.obj  <- regsubsets(birds~., catdat.train, nvmax = n)

# Save summary obj
sum <-  summary(bss.obj)


# Adjusted Rˆ2, C_p, BIC and CV prediction error
par(mfrow=c(2,2))


# adjRsq 
plot(sum$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type= "l")
bss.adjr2 = which.max(sum$adjr2)
points(bss.adjr2,sum$adjr2[bss.adjr2], col="red",cex=2,pch = 20)

# Cp
plot(sum$cp, xlab="Number of Variables", ylab="Cp", type='l')
bss.cp <- which.min(sum$cp)
points(bss.cp, sum$cp[bss.cp], col="red", cex=2, pch=20)

#BIC
bss.bic <- which.min(sum$bic)
plot(sum$bic, xlab="Number of Variables", ylab="BIC", type='l')
points(bss.bic, sum$bic[bss.bic], col="red", cex=2, pch=20)


# Cross-validated prediction error. 
# Create a prediction function to make predictions for regsubsets with id predictors included.
predict.regsubsets <- function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

# Create indices to divide the data between folds.
k <- 10
folds <-  sample(1:k, nrow(catdat.train), replace=TRUE)
cv.errors <- matrix(NA, k, n, dimnames=list(NULL, paste(1:n)))

# Perform CV.
for(j in 1:k){
  bss.obj<- regsubsets(birds~., data = catdat.train[folds != j,],nvmax = n)
  for(i in 1:n){
    pred <- predict(bss.obj, catdat.train[folds==j,], id=i) # Skal ikke noe test-data brukes her? NEI! test dataen brukes vel bare helt til slutt?
    cv.errors[j,i] <-  mean( (catdat.train$birds[folds==j]-pred)^2)
  }
}

# Compute mean cv errors for each model size.
mean.cv.errors=apply(cv.errors,2,mean)
bss.cv <- which.min(mean.cv.errors) 

# Plot the mean cv errors.
plot(mean.cv.errors, xlab = "Number of Variables", ylab = "CV", type='l')
points(bss.cv, mean.cv.errors[bss.cv], col="red", cex=2, pch=20)

par(mfrow=c(1,1))

# Best model, based on amount of variables chosen by cv.
coef(bss.obj, bss.cv) # Selected variables.
fit <- lm(birds~weight+wetfood+daily.playtime+children.13+urban+bell+daily.outdoortime, data = catdat.train)
# Kan man få ut den beste modellen fra reg.best i stedet for å fit på nytt?
# Trur ikkje det :(
pred.regbest <- predict(fit, newdata = catdat.test)
mse.regbest <- mean((pred.regbest - catdat.test$birds)^2) # Test Mean Square Error.
mse.regbest
```

The optimal number of predictors with respect to  $R_\mathrm{adj}^2$ is `r bss.adjr2`, for $BIC$ it is `r bss.bic`, for $C_p$ it is `r bss.cp` and for cross validated prediction error it is `r bss.cv`. We would argue that cross-validation is a more reliable way of selecting among the models, since it is a resampling method, compared to only running one best subset selection on the training data and choosing based on the other model selection criteria. Hence, the selected variables are as shown below. The test MSE is calculated as `r mse.regbest`.

## c)
Using Lasso regression on the same data set leads to the following. In order to choose an optimal value of $\lambda$, cross-validation is used (10-fold). 

```{r}
x.train <- model.matrix(birds~., data = catdat.train)[, -1]
y.train <- catdat.train$birds
x.test <- model.matrix(birds~., data = catdat.test)[, -1]
y.test <- catdat.test$birds

grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x.train, y.train, alpha = 1, lambda = grid) 
# alpha = 1 specifies Lasso regression. 

# Perform cross validation. 
set.seed(4268)
cv.out <- cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam # Lambda ble veldig lik least squares (0)!

# Report the non-zero coefficients. Refit model on entire data-set, with lambdabest.
# Why do you refit on the entire dataset?? We need the test set to compute MSE afterwards??
#x <- model.matrix(birds~., data = catdat)
#y <- catdat$birds
#full.lasso <- glmnet(x, y, alpha = 1) # Skal lambda = grid være med her?
# Estimatene blir litt annerledes avhengig av om lamba = grid er med eller ikke!
#lasso.coeff <- predict(lasso.mod, type = "coefficients", s = bestlam)
#lasso.coeff

# Think this is a better way to display coefficients.
lasso.coeff = coef(cv.out, s = "lambda.min")
lasso.coeff

# Non-zero coefficients.
# Trenger ikke denne vel?
lasso.coeff[lasso.coeff != 0]

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x.test)
mse.lasso <- mean((lasso.pred - y.test)^2)
mse.lasso
```

As seen in the output above, the test MSE is `r mse.lasso`. Moreover, all coefficients, except `fellow.cats` and `owner.age` are non-zero. The best lambda according to the cross-validation is close to 0, which means that the Lasso regression model is very similar to the linear least squares model, which is confirmed by the fact that only two coefficients are non-zero. \textcolor{red}{Hvorfor er det to intercept-verdier ovenfor? (én er null og den andre har et estimat!)} \textcolor{green}{tror jeg fiksa det. stilte også spm på piazza om det :)}

## d)
When $\lambda \longrightarrow \infty$, the Lasso regression gives the null model, i.e. the model where all the coefficients are zero. When $\lambda = 0$, the Lasso regression simply gives the least squares fit. 

## e)
 i)
```{r}
# A model with only intercept.
only.intercept <- lm(birds~1, data = catdat.train)
yhat.intercept <- predict(only.intercept, newdata = catdat.test)
mse.intercept <- mean((yhat.intercept - catdat.test$birds)^2) # Test Mean Square Error.
mse.intercept

# This should be the same as predicting each y has the mean of the training data.
# This is a better way of doing it IMO!
mse.intercept2 <- mean((mean(catdat.train$birds)-catdat.test$birds)^2) 
mse.intercept2
```
ii)
```{r}
# A simple linear regression. Hvilken kovariat skal vi velge da? Antar at de mener vanlig multippel regresjon, full modell?
least.sq <- lm(birds~., data = catdat.train)
yhat.least.sq <- predict(least.sq, newdata = catdat.test)
mse.least.sq <- mean((yhat.least.sq - catdat.test$birds)^2) # Test Mean Square Error.
mse.least.sq
```
We see that using only an intercept yields an MSE of `r mse.intercept`, while a standard linear regression yields an MSE of `r mse.least.sq`. On the other hand, best subset selection resulted in a test MSE of `r mse.regbest` and lasso regression resultet in a test MSE of `r mse.lasso`. We see that both models form a) and b) are superier to the ones introduced here, with regard to the test MSE.


## f)

A table with the test MSE values from best subset selection, lasso regression, intercept-only and ordinary linear regression is shown below. 

```{r}
# Code from exercise9 for making the table (after we are done with all the models above.)
msrate <- rbind(c(mse.regbest), c(mse.lasso), c(mse.intercept), c(mse.least.sq))
rownames(msrate) <- c("Best Subset", "Lasso", "Intercept-only", "Least Squares")
colnames(msrate) <- c("Test MSE")
msrate
```

We observe that the intercept-only model is clearly the worst, which is as expected, since it predicts the mean of the `birds`-values always. Moreover, the best subset is best by a small margin, while lasso and least squares are almost equal in MSE, where lasso is slightly better. As expected, the regularization methods give better results compared to standard least squares. 

# Problem 2

## a) 

TRUE, TRUE, FALSE, FALSE (Tror den har 7 basis-funksjoner.)  

\textcolor{red}{Usikker på om (ii) er et lurespm eller ikke.}

## b) 

The basis functions for a cubic spline with knots at the quartiles $q_1$ and $q_2$ of variable $X$ are 

$$
X, X^2, X^3, (X-q_1)^3 \text{ and } (X-q_2)^3.
$$

## c)

(i)
```{r}
# First plot birds against daily.outdoortime.
#plot(birds~daily.outdoortime, data = catdat)

# Fit polynomial regression with training data.
attach(catdat.train)
time.range <- range(daily.outdoortime)
time.grid <- seq(from = time.range[1], to = time.range[2])
plot(birds~daily.outdoortime, data = catdat, main = "Polynomial Regression", xlab = "Outdoor Time", ylab="Birds")
deg <- 10
col <- seq(from = 10, to = deg + 10) 
mse.train <- seq(from = 1, to = deg)
for (d in 1:deg){
  fit <- lm(birds~poly(daily.outdoortime, d), data = catdat.train)
  preds <- predict(fit, newdata = list(daily.outdoortime = time.grid))
  lines(time.grid, preds, type = "l", col = col[d], pch = 20)
  mse.train[d] = mean((preds - catdat.train$birds)^2)
}
legend(60, 540, legend=seq(from = 1, to = deg), col = col, pch = 20)

mse.train # MSE from training data.
which.min(mse.train)
```

(ii) The MSE on the training data is reported above. The MSE is lowest for the regular linear model, then it increases and fluctuates around a relatively stable value. The explanation for this is that ... \textcolor{red}{Hva kan forklaringen på dette være?}


# Problem 3

## a)

TRUE, TRUE, FALSE, FALSE (velger vel alltid den minste modellen, da jeg synes de tre siste ser like gode ut. )

## b)

Based solely on the tree above, a tree pruned down to three leaves should consist of `age < 46.5`, `country: indonesia, japan, Korea` and `country: France`. The reason behind this is that these keep the splits that are futhest to the top in the tree, which signals that these are the most important splits (they differentiate the most of the data compared to all other splits). \textcolor{red}{Dette må finskrives, men tror du skjønner poenget ;) Kan ikke konse skikkelig i dag. Kan også tegne en figure etter hvert hvis vi ønsker det ;)}

## c)

```{r}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # Google file ID.
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.train <-d.diabetes$ctrain
d.test <- d.diabetes$ctest
d.train$diabetes <- as.factor(d.train$diabetes) # La til denne for å få til klassifisering?!
d.test$diabetes <- as.factor(d.test$diabetes) # La til denne for å få til klassifisering?!
```

(i)

```{r}
# Simple classification tree.
set.seed(1)
simple.tree <- tree(diabetes~., data = d.train)
summary(simple.tree)
plot(simple.tree)
text(simple.tree, pretty = 1)

# Calculate classification error for the unpruned tree.
yhat.simple <- predict(simple.tree, newdata = d.test, type = "class")
conf.table.simple <- table(yhat.simple, d.test$diabetes)
conf.table.simple
class.rate.simple <-  1-sum(diag(conf.table.simple))/(sum(conf.table.simple))
class.rate.simple

# Apply cost complexity pruning. 
cv.diabetes <- cv.tree(simple.tree, FUN = prune.misclass) # 10-fold cv is default. Vet ikke om vi burde bruke prune.misclass!?
# Å bruke default, som er deviance, gir helt andre resultater!
best <- which.min(cv.diabetes$dev) # Finds best model.
plot(cv.diabetes$size, cv.diabetes$dev, type = "b")
points(cv.diabetes$size[best], cv.diabetes$dev[best], col = "red", pch = 20)
#plot(cv.diabetes$k, cv.diabetes$dev, type = "b") # Vet ikke helt hva k er?
#best.tree <- prune.tree(simple.tree, best = cv.diabetes$size[best]) # Bruk hvis deviance brukes over.
best.tree <- prune.misclass(simple.tree, best = 6)
summary(best.tree)
plot(best.tree)
text(best.tree, pretty = 1)

# Calculate classification error for the pruned tree.
yhat.pruned <- predict(best.tree, newdata = d.test, type = "class")
conf.table.pruned <- table(yhat.pruned, d.test$diabetes)
conf.table.pruned
class.rate.pruned <-  1-sum(diag(conf.table.pruned))/(sum(conf.table.pruned))
class.rate.pruned
```

Despite the fact that the largest tree is the best, we choose to prune the tree to 6 leaves, since this is not that much worse and is much more interpretable than the larger model with 17 leaves. 

Also, we can se that the misclassification error is reported below. This is very similar for the unpruned and the pruned tree, which suggests that a more advanced method perhaps will make larger improvements on the simple classification tree. 

(ii)

A more advanced method for constructing the classification tree could be bagging, random forests or boosting. Which should we choose and why?

```{r}

```


# Problem 4

## a)

TRUE, Perhaps (usikker her), FALSE (usikker), TRUE

## b)

```{r}
#  id <- "1x_E8xnmz9CMHh_tMwIsWP94czPa1Fpsj"  # google file ID
#  d.leukemia <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)
# set.seed(2399)
# t.samples <- sample(1:60, 15, replace = F)
# d.leukemia$Category <- as.factor(d.leukemia$Category)
# d.leukemia.test <- d.leukemia[t.samples, ]
# d.leukemia.train <- d.leukemia[-t.samples, ]
```

(i) A support vector machine (SVM) is more suitable than a logistic regression here, because \textcolor{red}{The data should be in clear boxes, which is hard for logistic regression? (utydelig skrevet, kaos i hodet)}



# Problem 5

## a)

TRUE, FALSE, FALSE, FALSE

## b)
