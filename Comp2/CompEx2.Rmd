---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 2: Group 39"
author: "Alexander J. Ohrt, Jim Totland"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---


```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
library(tidyverse)
library(ggplot2)
library(ggfortify)
library(leaps)
library(glmnet)
library(tree)
library(caret)
library(randomForest)
library(readr)
library(e1071)
library(dplyr)
library(gbm)
library(MASS)

knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=5, fig.height=5, comment = "#>", fig.align = "center")
```

# Problem 1
## a)
FALSE, TRUE, TRUE, FALSE

## b)
Best subset selection is used to identify a satisfactory model that uses a subset of the variables. Plots for $R_\mathrm{adj}^2$, $BIC$, $C_p$ and cross validated prediction error as model selection criteria are made. The red dots show the best amount of variables used in the subset, according to each criterion. \textcolor{red}{Dersom vi får for mange sider: Tenker at vi kan ta bort alle disse andre former for model selection criteria og kun ha med cv. Kan da også ev kommentere på at vi har testet disse tre andre metodene og at de alle gir ulike modeller med ulike kovariater, samt høyere test MSE i dette tilfellet (Dette har jeg testet for alle sammen ;))}

```{r}
id <- "1iI6YaqgG0QJW5onZ_GTBsCvpKPExF30G"  # Google file ID.
catdat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), 
                   header = T)

set.seed(4268)
train.ind = sample(1:nrow(catdat), 0.5 * nrow(catdat))
catdat.train = catdat[train.ind, ]
catdat.test = catdat[-train.ind, ]

# Perform best subset selection using all the predictors and the training data.
n <- ncol(catdat.train) - 1 # Number of predictors.
bss.obj <- regsubsets(birds~., catdat.train, nvmax = n) # Best subset selection. 

# Save summary obj.
sum <-  summary(bss.obj)

# Adjusted Rˆ2, C_p, BIC and CV prediction error.
par(mfrow=c(2,2)) # Adjust settings for plotting. 

# adjRsq. 
plot(sum$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type= "l")
bss.adjr2 = which.max(sum$adjr2)
points(bss.adjr2,sum$adjr2[bss.adjr2], col="red",cex=2,pch = 20)

# Cp.
plot(sum$cp, xlab="Number of Variables", ylab="Cp", type='l')
bss.cp <- which.min(sum$cp)
points(bss.cp, sum$cp[bss.cp], col="red", cex=2, pch=20)

# BIC.
bss.bic <- which.min(sum$bic)
plot(sum$bic, xlab="Number of Variables", ylab="BIC", type='l')
points(bss.bic, sum$bic[bss.bic], col="red", cex=2, pch=20)

# Cross-validated prediction error. 
# Create a prediction function to make predictions for regsubsets with id predictors included.
predict.regsubsets <- function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

# Create indices to divide the data between folds.
k <- 10
folds <-  sample(1:k, nrow(catdat.train), replace=TRUE)
cv.errors <- matrix(NA, k, n, dimnames=list(NULL, paste(1:n)))

# Perform CV.
for(j in 1:k){
  bss.obj <- regsubsets(birds~., data = catdat.train[folds != j,],nvmax = n)
  for(i in 1:n){
    pred <- predict(bss.obj, catdat.train[folds==j,], id=i)
    cv.errors[j,i] <- mean((catdat.train$birds[folds==j]-pred)^2)
  }
}

# Compute mean cv errors for each model size.
mean.cv.errors <- apply(cv.errors,2,mean)
bss.cv <- which.min(mean.cv.errors) 

# Plot the mean cv errors.
plot(mean.cv.errors, xlab = "Number of Variables", ylab = "CV", type='l')
points(bss.cv, mean.cv.errors[bss.cv], col="red", cex=2, pch=20)
```

The optimal number of predictors (in addition to the intercept) with respect to $R_\mathrm{adj}^2$ is `r bss.adjr2`, for $BIC$ it is `r bss.bic`, for $C_p$ it is `r bss.cp` and according to the cross validated prediction error it is `r bss.cv`. We would argue that cross-validation is a more reliable way of selecting among the models, since it is a resampling method, compared to only running one best subset selection on the training data and choosing based on the other model selection criteria. Hence, the selected variables are as shown below.

```{r}
par(mfrow=c(1,1)) # Adjust settings for plotting. 
bss.obj2 <- regsubsets(birds~., data = catdat.train, nvmax = n)
# Best model, based on amount of variables chosen by cv.
coef(bss.obj2, bss.cv) # Selected variables.
fit <- lm(birds~weight+wetfood+daily.playtime+children.13+urban+bell+daily.outdoortime, data = catdat.train)
# Kan man få ut den beste modellen fra reg.best i stedet for å fit på nytt?
# Se øving 6 for hvordan dette kan gjøres! Kan også bare la det stå sånn tenker jeg. 
pred.regbest <- predict(fit, newdata = catdat.test)
mse.regbest <- mean((pred.regbest - catdat.test$birds)^2) # Test Mean Square Error.
mse.regbest
```

The test MSE is calculated as `r mse.regbest`.

## c)
Using Lasso regression on the same data set leads to the following. In order to choose an optimal value of $\lambda$, 10-fold cross-validation is used. 

```{r, fig.height = 3.5}
x.train <- model.matrix(birds~., data = catdat.train)[, -1]
y.train <- catdat.train$birds
x.test <- model.matrix(birds~., data = catdat.test)[, -1]
y.test <- catdat.test$birds

lasso.mod <- glmnet(x.train, y.train, alpha = 1) 
# alpha = 1 specifies Lasso regression. 

# Perform cross validation. 
set.seed(4268)
cv.out <- cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam # Lambda is small, so the fit is relatively similar to least squares. 

lasso.coeff <- coef(cv.out, s = "lambda.min")
lasso.coeff

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x.test)
mse.lasso <- mean((lasso.pred - y.test)^2)
mse.lasso
```

As seen in the output above, the test MSE is `r mse.lasso`. Moreover, all coefficients, except `sex`, `age`, `fellow.cats` and `daily.catnip` are non-zero. The best lambda according to the cross-validation is relatively close to 0, which suggests that the Lasso regression model is likely similar to the linear least squares model. This is confirmed by the fact that four coefficients are zero, while the rest are non-zero, which means that some variable selection has been made.  

## d)
When $\lambda \longrightarrow \infty$, the Lasso regression gives the null model, i.e. the model where all the coefficients are zero. When $\lambda = 0$, the Lasso regression simply gives the least squares fit. 

## e)
 i) A model with only intercept always predicts the mean of the response in the training data. Hence, the test MSE for this model is calculated as below. 
```{r}
mse.intercept <- mean((mean(catdat.train$birds)-catdat.test$birds)^2) 
mse.intercept
```
ii)
```{r}
# A multiple linear regression.
least.sq <- lm(birds~., data = catdat.train)
yhat.least.sq <- predict(least.sq, newdata = catdat.test)
mse.least.sq <- mean((yhat.least.sq - catdat.test$birds)^2) 
mse.least.sq
```
We see that using only an intercept yields a test MSE of `r mse.intercept`, while a standard linear regression yields a test MSE of `r mse.least.sq`. On the other hand, best subset selection resulted in a test MSE of `r mse.regbest` and lasso regression resulted in a test MSE of `r mse.lasso`. We see that both models from a) and b) are superior to the ones introduced here, in terms the test MSE.

## f)

A table with the test MSE values from best subset selection, lasso regression, intercept-only and ordinary linear regression is shown below. 

```{r}
# Code from exercise9 for making the table (after we are done with all the models above.)
msrate <- rbind(c(mse.regbest), c(mse.lasso), c(mse.intercept), c(mse.least.sq))
rownames(msrate) <- c("Best Subset", "Lasso", "Intercept-only", "Least Squares")
colnames(msrate) <- c("Test MSE")
msrate
```

We observe that the intercept-only model is clearly the worst, which is as expected, since it always predicts the mean of the `birds`-values. Moreover, best subset selection yields the best results by a small margin, while lasso and least squares are almost equal in MSE, where lasso is slightly better. The regularization methods give better results compared to standard least squares. The results are as expected. First of all, best subset selection is an exhaustive search on all possible models with the covariates, which means that it should find the best possible linear model. This could lead to overfitting, because of the large search space, but has yielded good results in this case. Secondly, shrinkage methods, like Lasso regression, are used to reduce overfitting to the training data. Then, the test MSE might be reduced, since the variance might have been reduced more than the bias has increased (bias-variance trade-off) after regularization. In this case, this has given good results, compared to the least squares regression. Finally, given the assumption that the covariates have a somewhat linear relationship to the response, it is expected that the linear regression has smaller test MSE compared to the intercept-only model. 

# Problem 2

## a) 

TRUE, TRUE, FALSE, FALSE. 

\textcolor{red}{Usikker på om (ii) er et lurespm eller ikke. Tenker at vi burde spørre om dette! Den penalizer to integralet til den dobbeltderiverte kvadrert. }

## b) 

The basis functions for a cubic spline with knots at the quartiles $q_1$ and $q_2$ of variable $X$ are 

$$
X, X^2, X^3, (X-q_1)^3_+ \text{ and } (X-q_2)^3_+, 
$$

where 

$$
  (X-q_j)^d_+ = \begin{cases}
    (x-q_j)^d &, x > q_j\\
    0&,\text{otherwise.} 
  \end{cases}
$$

## c)

(i)
```{r, fig.height=3.7}
# Fit polynomial regression with training data.
outdoor.data <- catdat.train[c("daily.outdoortime", "birds")] # Pick out necessary training data. 
par(mar=c(4, 3, 3, 4.5), xpd=TRUE) # Change margins of plots. 
plot(outdoor.data, 
     main = "Polynomial Regression", xlab = "Outdoor Time", ylab="Birds")
deg <- 1:10
col <- rainbow(n = length(deg))
mse.train <- seq(from = 1, to = length(deg))

for (d in 1:length(deg)){ # Kan også bruke en sapply her (se øving 7).
  fit <- lm(birds~poly(daily.outdoortime, d), data = outdoor.data)
  lines(sort(outdoor.data[, 1]), fit$fit[order(outdoor.data[, 1])], col = col[d])
  mse.train[d] = mean((predict(fit, outdoor.data) - outdoor.data[, 2])^2)
}

legend("topright", inset = c(-0.25, 0), legend=paste("d = ", deg), col = col, pch = 20)

mse.train # MSE from training data.
which.min(mse.train)
```

(ii) We see that the training MSE decreases steadily as the degree of the polynomial regression increases. This is because we increase the flexibility of the model when we increment the polynomial degree. This increases the likelihood of overfitting, i.e. reduces the training MSE. 


# Problem 3

## a)

TRUE, TRUE, FALSE, FALSE 

## b)

Based solely on the tree above, a tree pruned down to three leaves should consist of `age < 81.5`, `country: indonesia, japan, Korea` and `country: France`. A figure is shown below, where the terminal nodes are denoted with each of their respective probabilities. The terminal nodes denoted by the names above are shown from left to right. This is the best way to prune the tree down to three leaves because this leads to the most coarse stratification of the covariate space. `age` and `country`, in that order, can be interpreted as the most important covariates when determining the probability of dying. \textcolor{red}{Men hvorfor skal den andre splitten til høyre beholdes og ikke den til venstre? Har vi en grunn til hvorfor dette?}


```{r, echo = F, fig.height=3}
set.seed(1)
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__" # Google file ID.
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
t.corona <- tree(deceased~., data = d.corona) 
plot(t.corona)
text(t.corona, pretty = 0)
```


## c)

```{r}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # Google file ID.
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.train <-d.diabetes$ctrain
d.test <- d.diabetes$ctest

# Spør om hvorfor dette må med!
d.train$diabetes <- as.factor(d.train$diabetes) # La til denne for å få til klassifisering?!
d.test$diabetes <- as.factor(d.test$diabetes) # La til denne for å få til klassifisering?!
```

(i)

```{r}
# Simple classification tree.
set.seed(1)
simple.tree <- tree(diabetes~., data = d.train, split = "deviance")
plot(simple.tree, type = "proportional")
text(simple.tree, cex = 0.5, pretty = 0)

# Calculate classification error for the unpruned tree.
yhat.simple <- predict(simple.tree, newdata = d.test, type = "class")
conf.table.simple <- table(yhat.simple, d.test$diabetes)
conf.table.simple
class.rate.simple <-  1 - 
  sum(diag(conf.table.simple))/(sum(conf.table.simple))
class.rate.simple

# Apply cost complexity pruning. 
cv.diabetes <- cv.tree(simple.tree, FUN = prune.misclass) # 10-fold cv is default. 
# Classification error rate is used to guide the pruning process, 
# since prediction accuracy of the final pruned tree is the goal. 
best <- which.min(cv.diabetes$dev) # Finds best model.
```

```{r, fig.height=3}
plot(cv.diabetes$size, cv.diabetes$dev, type = "b", xlab = "Terminal Nodes", ylab = "Misclassifications")
points(cv.diabetes$size[best], cv.diabetes$dev[best], col = "red", pch = 20)

best.tree <- prune.misclass(simple.tree, best = 6)
plot(best.tree)
text(best.tree, cex = 0.7, pretty = 0)
```

```{r}
# Calculate classification error for the pruned tree.
yhat.pruned <- predict(best.tree, newdata = d.test, type = "class")
conf.table.pruned <- table(yhat.pruned, d.test$diabetes)
conf.table.pruned
class.rate.pruned <-  1-sum(diag(conf.table.pruned))/(sum(conf.table.pruned))
class.rate.pruned
```



Despite the fact that the largest tree is the best, according to the cross-validation error, we choose to prune the tree to 6 leaves, since it is much more interpretable than the larger model with 17 leaves. 
\textcolor{red}{Synes det er litt rart at den mest komplekse modellen gir best CV-error. Men poenget med oppgaven er kanskje at vi skal prioritere tolkning og enkelhet? SPM HER!} \textcolor{green}{Tror kanskje at det er overfitting her? Siden test-error-raten er lavere for pruned tree, til tross for at cv error sier at den burde vært større. }

The misclassification error for the pruned tree is reported above. Despite the fact that the CV-error is best for the largest model, it is apparent that the misclassification error on the test set is slightly lower for the pruned tree with 6 terminal nodes, compared to the unpruned tree. Despite this, the errors are very similar, which suggests that a more advanced method should be tested. 

(ii) \textcolor{red}{SPM: Hvorfor bruke random forest her, og ikke bagging eller boosting?}
For the more advanced method, a random forest approach is considered. The number of trees is chosen to be $B = 1000$, since this amount seems to be sufficient for the test error to settle. Furthermore, since $p = 7$, we choose $m = 3 \approx \sqrt{p}$, because this is recommended in ISLR. \textcolor{red}{SPM: spør om hvor mye rettferdiggjørelse som kreves her!}

```{r}
set.seed(1)
B <- 1000
m <- 3
rf.d <- randomForest(diabetes ~ ., d.train, mtry = m, ntree = B, importance = T)

yhat <- predict(rf.d, newdata = d.test, type = "class")
response.test <- d.test$diabetes
d.misclass <- table(yhat, response.test)
d.misclass
rf.misclass <- 1 - sum(diag(d.misclass))/sum(d.misclass)
rf.misclass
```
The resulting test misclassification rate is `r rf.misclass`. This is slightly lower than for the simple tree. Next, we find the most influential variables in this classification problem.
```{r, fig.height=3.5}
# Find the most important variables.
varImpPlot(rf.d, main = "")
```
Based on both metrics in the plots above, it is apparent that `glu` and `bmi` are the most influential, as expected based on the simple trees from earlier. 

# Problem 4

## a)

TRUE, TRUE \textcolor{red}{Jeg tenker TRUE her, siden variansen senkes med soft-margin, slik at overfitting kan bli mindre.} \textcolor{green}{Ja, det skal vel være lett å finne en separating hyperplane siden p >> n, så kanksje FALSE er riktig?}, FALSE (usikker, \textcolor{green}{logreg bryter jo sammen når p >n, men det finnes vel ridge-metoder for logreg som gjør at det funker?} \textcolor{red}{Siden logreg bryter sammen så regner jeg med at svaret er FALSE. Tror ikke de regner med andre metoder for optimering, da vi egt ikke har gjennomgått dette.}), TRUE

## b)

```{r 4b}
id <- "1x_E8xnmz9CMHh_tMwIsWP94czPa1Fpsj"  # Google file ID.
d.leukemia <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)
set.seed(2399)
t.samples <- sample(1:60, 15, replace = F)
d.leukemia$Category <- as.factor(d.leukemia$Category)
d.leukemia.test <- d.leukemia[t.samples, ]
d.leukemia.train <- d.leukemia[-t.samples, ]
```

(i) A support vector machine (SVM) is more suitable than a logistic regression here, because $p > n$. This means that the parameters in the logistic regression model cannot be estimated. Linear or quadratic discriminant analysis could also be used in this case, in addition to regularized logistic regression variants, i.e. with added penalty term when optimizing (similar to e.g. ridge regression). \textcolor{green}{Kan bagging/randomForests/boosting bruker her også?}

(ii) The paper intends to demonstrate a new method for identifying the subset of genes (predictors) which capture the necessary information in order to classify patients into different groups (response). They introduce the Ensemble SVM-Recursive Feature Elimination (ESVM-RFE), which combines the ensemble and bagging methods used in random forests with SVMs and a backward elimination strategy.

(iii)

```{r}
svcfit <- svm(Category~., data = d.leukemia.train, kernel = "linear", cost = 1, scale = T)

# Confusion table from training. 
train.yhat <- predict(svcfit, newdata = d.leukemia.train)
train.misclass <- table(predict = train.yhat, truth = d.leukemia.train$Category)
train.misclass
# Misclassification error rate from training. 
1 - sum(diag(train.misclass)) / sum(train.misclass)

# Confusion table from testing. 
test.yhat <- predict(svcfit, newdata = d.leukemia.test) 
test.misclass <- table(predict = test.yhat, truth = d.leukemia.test$Category)
test.misclass
# Misclassification error rate from testing.
1 - sum(diag(test.misclass)) / sum(test.misclass)
```


The misclassification error rates are `r 1 - sum(diag(train.misclass)) / sum(train.misclass)` for the training data and `r 1 - sum(diag(test.misclass)) / sum(test.misclass)` for the test data.

The training error rate is not surprising, as finding a separating hyperplane when $p >> n$ is easy \textcolor{red}{SPM: Men lav cost betyr høy margin. Er C = 1 en lav eller høy cost her?}. The support vector classifier has 1 false positive and 4 false negatives on the test set. This means that a false negative is the most common type of error that is seen in the test set. 

Despite this, the classification method is successful, since it performs slightly better than always predicting the most frequently occurring class, which is `Non-Relapse`. The output below substantiates this argument. 

```{r}
# Most occurring class is "Non-Relapse".
summary(d.leukemia.train$Category)
# Confusion table from predicting most occuring class. 
test.misclass.most <- table(predict = rep("Non-Relapse", length(d.leukemia.test$Category)), truth = d.leukemia.test$Category)
test.misclass.most
# Misclassification error rate from testing.
1 - sum(diag(test.misclass.most)) / sum(test.misclass.most)
```

(iv)
First, the analysis is repeated with $\gamma = 10^{-2}$. 
```{r}
svcfit.radial.1 <- svm(Category~., data = d.leukemia.train, kernel = "radial", cost = 1, gamma = 1e-3, scale = T)

# Confusion table from training. 
train.yhat.radial.1 <- predict(svcfit.radial.1, newdata = d.leukemia.train)
train.misclass.radial.1 <- table(predict = train.yhat.radial.1, truth = d.leukemia.train$Category)
train.misclass.radial.1
# Misclassification error rate from training. 
1 - sum(diag(train.misclass.radial.1)) / sum(train.misclass.radial.1)

# Confusion table from testing. 
test.yhat.radial.1 <- predict(svcfit.radial.1, newdata = d.leukemia.test) 
test.misclass.radial.1 <- table(predict = test.yhat.radial.1, truth = d.leukemia.test$Category)
test.misclass.radial.1
# Misclassification error rate from testing.
1 - sum(diag(test.misclass.radial.1)) / sum(test.misclass.radial.1)
```

The training error rate is still 0 in this case. \textcolor{red}{Interpretation sammenlignet med tidligere; hvorfor? Hvordan påvirker gamma dette? Dette må sjekkes ut.}. The test misclassification error rate is no better than always predicting the most frequently occurring class in the training data. Hence, this classifier is not successful. 

Next, the analysis is repeated with $\gamma = 10^{-5}$. 

```{r}
svcfit.radial.2 <- svm(Category~., data = d.leukemia.train, kernel = "radial", cost = 1, gamma = 1e-6, scale = T)

# Confusion table from training. 
train.yhat.radial.2 <- predict(svcfit.radial.2, newdata = d.leukemia.train)
train.misclass.radial.2 <- table(predict = train.yhat.radial.2, truth = d.leukemia.train$Category)
train.misclass.radial.2
# Misclassification error rate from training. 
1 - sum(diag(train.misclass.radial.2)) / sum(train.misclass.radial.2)

# Confusion table from testing. 
test.yhat.radial.2 <- predict(svcfit.radial.2, newdata = d.leukemia.test) 
test.misclass.radial.2 <- table(predict = test.yhat.radial.2, truth = d.leukemia.test$Category)
test.misclass.radial.2
# Misclassification error rate from testing.
1 - sum(diag(test.misclass.radial.2)) / sum(test.misclass.radial.2)
```
The training error rate is not 0 in this case. It is apparent that no individuals are correctly predicted to relapse. \textcolor{red}{Interpretation sammenlignet med tidligere; hvorfor?}. The test misclassification error rate is no better than always predicting the most frequently occurring class in the training data. Hence, this classifier is not successful. 

## c)
$$
\begin{split}
K(\boldsymbol{x}_i, \boldsymbol{x}_i') &= (1 + \sum_{j=1}^2x_{ij}x_{ij}')^2 \\ &= (x_{i1}x_{i1}' + x_{i2}x_{i2}' + 1)^2 \\
&= (x_{i1}x_{i1}')^2 + (x_{i2}x_{i2}')^2 + 2x_{i1}x_{i2}\cdot x_{i1}'x_{i2}' + 2x_{i1}x_{i1}' + 2x_{i2}x_{i2}' + 1 \\
&= \langle h(\boldsymbol{x}_i), h(\boldsymbol{x}_i')\rangle,
\end{split}
$$
where
$$
h(\boldsymbol{x}_i) = (x_{i1}^2, x_{i2}^2,\sqrt{2}x_{i1}x_{i2}, \sqrt{2}x_{i1}, \sqrt{2}x_{i2}, 1)^T.
$$

# Problem 5

## a)

TRUE, FALSE, FALSE\textcolor{red}{hva menes med robust da?} \textcolor{green}{Alex: Her tenker jeg at de lurer på om resultatet av K-means clustering avhenger sterkt av den initielle randomiseringen av punktene i ulike clusters. Jeg tenker at dette er FALSE, nettopp fordi algoritmen kjøres for mange ulike randomiseringer, før den beste velges. Dette viser jo at den avhenger av det tilfeldige valget i starten, og er dermed ikke robust mot dette valget. Vi kan også spørre om dette er rett tolkning eventuelt ;)}, FALSE

## b)

(i)
```{r, fig.height=3}
set.seed(1)
x1 <- c(1, 2, 0, 4, 5, 6)
x2 <- c(5, 4, 3, 1, 1, 2)

K <- 2
cluster <-  sample(1:K, length(x1), replace = T)
df <- as.data.frame(cbind(x1, x2, cluster))
df$cluster <-  as.factor(df$cluster)

plt <- ggplot() +
        geom_point(data = df, aes(x1, x2, color = cluster)) +
        theme_bw() + scale_color_identity(guide = "legend", labels = c(1, 2))
plt
```

(ii) 
```{r, fig.height=3}
calc_centr <- function(k, df) {
  feats <- df[df$cluster == k, 1:2]
  mean.1 <- mean(feats[, 1])
  mean.2 <- mean(feats[, 2])
  return(c(mean.1, mean.2))
}

c1 <- calc_centr(1, df)
c2 <- calc_centr(2, df)
centroids <- as.data.frame(cbind(rbind(c1,c2), c(1,2), c("black","red")))
colnames(centroids) <-c("x", "y", "cluster", "color_code")
rownames(centroids) <- c(1,2)
centroids <- as.data.frame(rbind(c1,c2)) # denne er her som safeguard foreløpig! Må bestemme om vi bør beholde det jeg har gjort eller ikke!
#centroid er det samme som centroids vha de tre linjene ovenfor. 
#centroid <- data.frame("x" = c(c1[1], c2[1]), "y" = c(c1[2], c2[2]), "cluster" = c(1,2), "color_code" = c("black","red"))
# Tried to get same colors across both plots, this is the closest I have come. 
plt + geom_point(data = centroid, aes(x, y, color = color_code), shape = 4) + 
    scale_color_identity(guide = "legend", labels = c(1, 2), breaks = centroid$cluster)
# Usikker på hvorfor shape endres til firkant i legend her, men jeg skjønner ikke ggplot helt så hva vet vel jeg. 
```

(iii)
```{r, fig.height=3}
eucl.dist <- function(x, y) {
  d <- length(x) # Dimension.
  sum = 0
  x
  y
  for(i in 1:d) {
    sum = sum + (x[i] - y[i])^2
  }
  return(sqrt(sum))
}

reassign <- function(df, centroids){
  n = nrow(df)
  for(i in 1:n) {
    dist1 <-  eucl.dist(df[i, 1:2], centroids[1, ])
    dist2 <- eucl.dist(df[i, 1:2], centroids[2, ])
    if(dist1 > dist2) {
    df[i, 3]  <- 2
    } else {
      df[i, 3] <- 1
    }
  }
  return(df)
}

df2 <- reassign(df, centroids) # denne må fikses dersom det skal fungere med den nye centroid-df!
ggplot(data = df2, aes(x1, x2, color = cluster) ) + 
  geom_point() +
  theme_bw() + scale_color_identity(guide = "legend", labels = c(1, 2))
```

## c)

```{r}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
                             id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData = t(GeneData)
GeneData <- scale(GeneData)
n = ncol(GeneData)
GeneData[,n]

hc.euc.complete <- hclust(dist(GeneData), method = "complete")
hc.euc.single <- hclust(dist(GeneData), method = "single")
hc.euc.average <- hclust(dist(GeneData), method = "average")
par(mfrow = c(1, 3))
plot(hc.euc.complete, main = "Complete Linkage", xlab = " ", sub = " ", cex = 0.5)
plot(hc.euc.single, main = "Single Linkage", xlab = " ", sub = " ", cex = 0.5)
plot(hc.euc.average, main = "Average Linkage", xlab = " ", sub = " ", cex = 0.5)

# Skjønner ikke helt hvorfor Gendata skal transponeres..
dd <- as.dist(1 - cor(t(GeneData)))
hc.cor.complete <- hclust(dd, method = "complete")
hc.cor.single <- hclust(dd, method = "single")
hc.cor.average <- hclust(dd, method = "average")
par(mfrow = c(1, 3))
plot(hc.cor.complete, cex.main = 0.7, main = "CL with Correlation-based Distance", xlab = " ", sub = " ", cex = 0.5)
plot(hc.cor.single, cex.main = 0.7, main = "SL with Correlation-based Distance", xlab = " ", sub = " ", cex = 0.5)
plot(hc.cor.average, cex.main = 0.7, main = "AL with Correlation-based Distance", xlab = " ", sub = " ", cex = 0.5)

```

## d)

```{r}
cutree(hc.euc.complete, 2)
cutree(hc.euc.single, 2)
cutree(hc.euc.average, 2)

cutree(hc.cor.complete, 2)
cutree(hc.cor.single, 2)
cutree(hc.cor.average, 2)
```
 \textcolor{red}{Every method classifies everything correctly!!?}

## e)

(i) \textcolor{red}{JEg tror ikke vi skal bruke biplot slik som jeg har gjort her, det blit jo bare ei suppe}
```{r}
pr.out <- prcomp(GeneData, scale = T)
biplot(pr.out, scale = 0, cex = 0.4)
```

(ii)
```{r}
pr.var <-  pr.out$sdev^2
prop <- sum(pr.var[1:5]) / sum(pr.var)
```
The proportion of variance explained by the first 5 PCs is `r round(100 * prop, 2)`%.`

## f)


