---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 2: Group 39"
author: "Alexander J. Ohrt, Jim Totland"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---


```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
library(tidyverse)
library(ggplot2)
library(ggfortify)
library(leaps)
library(glmnet)
library(tree)
library(caret)
library(randomForest)
library(readr)
library(e1071)
library(dplyr)
library(gbm)
library(MASS)

knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=5, fig.height=5, comment = "#>", fig.align = "center")
```

# Problem 1
## a)
FALSE, TRUE, TRUE, FALSE

## b)
Best subset selection is used to identify a satisfactory model that uses a subset of the variables. Plots for $R_\mathrm{adj}^2$, $BIC$, $C_p$ and cross validated prediction error as model selection criteria are made. The red dots show the best amount of variables used in the subset, according to each criterion. \textcolor{red}{Dersom vi får for mange sider: Tenker at vi kan ta bort alle disse andre former for model selection criteria og kun ha med cv. Kan da også ev kommentere på at vi har testet disse tre andre metodene og at de alle gir ulike modeller med ulike kovariater, samt høyere test MSE i dette tilfellet (Dette har jeg testet for alle sammen ;))}

```{r}
id <- "1iI6YaqgG0QJW5onZ_GTBsCvpKPExF30G"  # Google file ID.
catdat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), 
                   header = T)

set.seed(4268)
train.ind = sample(1:nrow(catdat), 0.5 * nrow(catdat))
catdat.train = catdat[train.ind, ]
catdat.test = catdat[-train.ind, ]

# Perform best subset selection using all the predictors and the training data.
n <- ncol(catdat.train) - 1 # Number of predictors.
bss.obj <- regsubsets(birds~., catdat.train, nvmax = n) # Best subset selection. 

# Save summary obj.
sum <-  summary(bss.obj)

# Adjusted Rˆ2, C_p, BIC and CV prediction error.
par(mfrow=c(2,2)) # Adjust settings for plotting. 

# adjRsq. 
plot(sum$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type= "l")
bss.adjr2 = which.max(sum$adjr2)
points(bss.adjr2,sum$adjr2[bss.adjr2], col="red",cex=2,pch = 20)

# Cp.
plot(sum$cp, xlab="Number of Variables", ylab="Cp", type='l')
bss.cp <- which.min(sum$cp)
points(bss.cp, sum$cp[bss.cp], col="red", cex=2, pch=20)

# BIC.
bss.bic <- which.min(sum$bic)
plot(sum$bic, xlab="Number of Variables", ylab="BIC", type='l')
points(bss.bic, sum$bic[bss.bic], col="red", cex=2, pch=20)

# Cross-validated prediction error. 
# Create a prediction function to make predictions for regsubsets with id predictors included.
predict.regsubsets <- function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

# Create indices to divide the data between folds.
k <- 10
folds <-  sample(1:k, nrow(catdat.train), replace=TRUE)
cv.errors <- matrix(NA, k, n, dimnames=list(NULL, paste(1:n)))

# Perform CV.
for(j in 1:k){
  bss.obj <- regsubsets(birds~., data = catdat.train[folds != j,],nvmax = n)
  for(i in 1:n){
    pred <- predict(bss.obj, catdat.train[folds==j,], id=i)
    cv.errors[j,i] <- mean((catdat.train$birds[folds==j]-pred)^2)
  }
}

# Compute mean cv errors for each model size.
mean.cv.errors <- apply(cv.errors,2,mean)
bss.cv <- which.min(mean.cv.errors) 

# Plot the mean cv errors.
plot(mean.cv.errors, xlab = "Number of Variables", ylab = "CV", type='l')
points(bss.cv, mean.cv.errors[bss.cv], col="red", cex=2, pch=20)
```

The optimal number of predictors (in addition to the intercept) with respect to $R_\mathrm{adj}^2$ is `r bss.adjr2`, for $BIC$ it is `r bss.bic`, for $C_p$ it is `r bss.cp` and according to the cross validated prediction error it is `r bss.cv`. We would argue that cross-validation is a more reliable way of selecting among the models, since it is a resampling method, compared to only running one best subset selection on the training data and choosing based on the other model selection criteria. Hence, the selected variables are as shown below.

```{r}
par(mfrow=c(1,1)) # Adjust settings for plotting. 
bss.obj2 <- regsubsets(birds~., data = catdat.train, nvmax = n)
# Best model, based on amount of variables chosen by cv.
coef(bss.obj2, bss.cv) # Selected variables.
fit <- lm(birds~weight+wetfood+daily.playtime+children.13+urban+bell+daily.outdoortime, data = catdat.train)
# Kan man få ut den beste modellen fra reg.best i stedet for å fit på nytt?
# Se øving 6 for hvordan dette kan gjøres! Kan også bare la det stå sånn tenker jeg. 
pred.regbest <- predict(fit, newdata = catdat.test)
mse.regbest <- mean((pred.regbest - catdat.test$birds)^2) # Test Mean Square Error.
mse.regbest
```

The test MSE is calculated as `r mse.regbest`.

## c)
Using Lasso regression on the same data set leads to the following. In order to choose an optimal value of $\lambda$, 10-fold cross-validation is used. 

```{r}
x.train <- model.matrix(birds~., data = catdat.train)[, -1]
y.train <- catdat.train$birds
x.test <- model.matrix(birds~., data = catdat.test)[, -1]
y.test <- catdat.test$birds

lasso.mod <- glmnet(x.train, y.train, alpha = 1) 
# alpha = 1 specifies Lasso regression. 

# Perform cross validation. 
set.seed(4268)
cv.out <- cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam # Lambda is small, so the fit is relatively similar to least squares. 

lasso.coeff <- coef(cv.out, s = "lambda.min")
lasso.coeff

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x.test)
mse.lasso <- mean((lasso.pred - y.test)^2)
mse.lasso
```

As seen in the output above, the test MSE is `r mse.lasso`. Moreover, all coefficients, except `sex`, `age`, `fellow.cats` and `daily.catnip` are non-zero. The best lambda according to the cross-validation is relatively close to 0, which suggests that the Lasso regression model is likely similar to the linear least squares model. This is confirmed by the fact that four coefficients are zero, while the rest are non-zero, which means that some variable selection has been made.  

## d)
When $\lambda \longrightarrow \infty$, the Lasso regression gives the null model, i.e. the model where all the coefficients are zero. When $\lambda = 0$, the Lasso regression simply gives the least squares fit. 

## e)
 i) A model with only intercept always predicts the mean of the response in the training data. Hence, the test MSE for this model is calculated as below. 
```{r}
mse.intercept <- mean((mean(catdat.train$birds)-catdat.test$birds)^2) 
mse.intercept
```
ii)
```{r}
# A multiple linear regression.
least.sq <- lm(birds~., data = catdat.train)
yhat.least.sq <- predict(least.sq, newdata = catdat.test)
mse.least.sq <- mean((yhat.least.sq - catdat.test$birds)^2) 
mse.least.sq
```
We see that using only an intercept yields a test MSE of `r mse.intercept`, while a standard linear regression yields a test MSE of `r mse.least.sq`. On the other hand, best subset selection resulted in a test MSE of `r mse.regbest` and lasso regression resulted in a test MSE of `r mse.lasso`. We see that both models from a) and b) are superior to the ones introduced here, in terms the test MSE.

## f)

A table with the test MSE values from best subset selection, lasso regression, intercept-only and ordinary linear regression is shown below. 

```{r}
# Code from exercise9 for making the table (after we are done with all the models above.)
msrate <- rbind(c(mse.regbest), c(mse.lasso), c(mse.intercept), c(mse.least.sq))
rownames(msrate) <- c("Best Subset", "Lasso", "Intercept-only", "Least Squares")
colnames(msrate) <- c("Test MSE")
msrate
```

We observe that the intercept-only model is clearly the worst, which is as expected, since it always predicts the mean of the `birds`-values. Moreover, best subset selection yields the best results by a small margin, while lasso and least squares are almost equal in MSE, where lasso is slightly better. The regularization methods give better results compared to standard least squares. The results are as expected. First of all, best subset selection is an exhaustive search on all possible models with the covariates, which means that it should find the best possible linear model. This could lead to overfitting, because of the large search space, but has yielded good results in this case. Secondly, shrinkage methods, like Lasso regression, are used to reduce overfitting to the training data. Then, the test MSE might be reduced, since the variance might have been reduced more than the bias has increased (bias-variance trade-off) after regularization. In this case, this has given good results, compared to the least squares regression. Finally, given the assumption that the covariates have a somewhat linear relationship to the response, it is expected that the linear regression has smaller test MSE compared to the intercept-only model. 

# Problem 2

## a) 

TRUE, TRUE, FALSE, FALSE. 

\textcolor{red}{Usikker på om (ii) er et lurespm eller ikke. Tenker at vi burde spørre om dette! Den penalizer to integralet til den dobbeltderiverte kvadrert. }

## b) 

The basis functions for a cubic spline with knots at the quartiles $q_1$ and $q_2$ of variable $X$ are 

$$
X, X^2, X^3, (X-q_1)^3_+ \text{ and } (X-q_2)^3_+, 
$$

where 

$$
  (X-q_j)^d_+ = \begin{cases}
    (x-q_j)^d &, x > q_j\\
    0&,\text{otherwise.} 
  \end{cases}
$$

## c)

(i)
```{r, fig.height=3.7}
# Fit polynomial regression with training data.
outdoor.data <- catdat.train[c("daily.outdoortime", "birds")] # Pick out necessary training data. 
par(mar=c(4, 3, 3, 4.5), xpd=TRUE) # Change margins of plots. 
plot(outdoor.data, 
     main = "Polynomial Regression", xlab = "Outdoor Time", ylab="Birds")
deg <- 1:10
col <- rainbow(n = length(deg))
mse.train <- seq(from = 1, to = length(deg))

for (d in 1:length(deg)){ # Kan også bruke en sapply her (se øving 7).
  fit <- lm(birds~poly(daily.outdoortime, d), data = outdoor.data)
  lines(sort(outdoor.data[, 1]), fit$fit[order(outdoor.data[, 1])], col = col[d])
  mse.train[d] = mean((predict(fit, outdoor.data) - outdoor.data[, 2])^2)
}

legend("topright", inset = c(-0.25, 0), legend=paste("d = ", deg), col = col, pch = 20)

mse.train # MSE from training data.
which.min(mse.train)
```

(ii) We see that the training MSE decreases steadily as the degree of the polynomial regression increases. This is because we increase the flexibility of the model when we increment the polynomial degree. This increases the likelihood of overfitting, i.e. reduces the training MSE. 


# Problem 3

## a)

TRUE, TRUE, FALSE \textcolor{red}{men cv brukes i kombinasjon med CCP?}, FALSE (velger vel alltid den minste modellen, da jeg synes de tre siste ser like gode ut. )

## b)

Based solely on the tree above, a tree pruned down to three leaves should consist of `age < 46.5`, `country: indonesia, japan, Korea` and `country: France`. The reason behind this is that these keep the splits that are furthest to the top in the tree, which signals that these are the most important splits (they differentiate the most of the data compared to all other splits). \textcolor{red}{Dette må finskrives, men tror du skjønner poenget ;) Kan ikke konse skikkelig i dag. Kan også tegne en figure etter hvert hvis vi ønsker det ;)}

## c)

```{r}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # Google file ID.
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.train <-d.diabetes$ctrain
d.test <- d.diabetes$ctest
d.train$diabetes <- as.factor(d.train$diabetes) # La til denne for å få til klassifisering?!
d.test$diabetes <- as.factor(d.test$diabetes) # La til denne for å få til klassifisering?!
```

(i)

```{r}
# Simple classification tree.
set.seed(1)
simple.tree <- tree(diabetes~., data = d.train)
#Kan kanskje fjerne summary?
# Teksten må eventuelt justers på i plottet
summary(simple.tree)
plot(simple.tree)
text(simple.tree, cex = 0.5, pretty = 0)

# Calculate classification error for the unpruned tree.
yhat.simple <- predict(simple.tree, newdata = d.test, type = "class")
conf.table.simple <- table(yhat.simple, d.test$diabetes)
conf.table.simple
class.rate.simple <-  1 - 
  sum(diag(conf.table.simple))/(sum(conf.table.simple))
class.rate.simple

# Apply cost complexity pruning. 
cv.diabetes <- cv.tree(simple.tree, FUN = prune.misclass) # 10-fold cv is default. Vet ikke om vi burde bruke prune.misclass!? Jo tror det er riktig
best <- which.min(cv.diabetes$dev) # Finds best model.
plot(cv.diabetes$size, cv.diabetes$dev, type = "b")
points(cv.diabetes$size[best], cv.diabetes$dev[best], col = "red", pch = 20)
#plot(cv.diabetes$k, cv.diabetes$dev, type = "b") # Vet ikke helt hva k er?
# k er samme som alpha i CCP.
#best.tree <- prune.tree(simple.tree, best = cv.diabetes$size[best]) # Bruk hvis deviance brukes over.
best.tree <- prune.misclass(simple.tree, best = 6)
summary(best.tree)
plot(best.tree)
text(best.tree, cex = 0.7, pretty = 0)

# Calculate classification error for the pruned tree.
yhat.pruned <- predict(best.tree, newdata = d.test, type = "class")
conf.table.pruned <- table(yhat.pruned, d.test$diabetes)
conf.table.pruned
class.rate.pruned <-  1-sum(diag(conf.table.pruned))/(sum(conf.table.pruned))
class.rate.pruned
```

Despite the fact that the largest tree is the best, we choose to prune the tree to 6 leaves, since this is not that much worse and is much more interpretable than the larger model with 17 leaves. 
\textcolor{red}{Synes det er litt rart at den mest komplekse modellen gir best CV-error. Men poenget med oppgaven er kanskje at vi skal prioritere tolkning og enkelhet? Synes vi bør spørre om dette.}

Also, we can se that the misclassification error is reported below. This is very similar for the unpruned and the pruned tree, which suggests that a more advanced method perhaps will make larger improvements on the simple classification tree. 

(ii)
For the more advanced method, we consider the random forest approach. We choose the number of trees, $B = 1000$, since this amount seems to be suffcient for the test error to settle. Furthermore, since $p = 7$, we choose $m = 3 \approx \sqrt{p}$, because this is recommended in ISLR. \textcolor{red}{usikker på hvor mye rettferdigjørelse de forventer her...}

```{r}
set.seed(1)
B = 1000
m = 3
rf.d = randomForest(diabetes ~ ., d.train, mtry = m, ntree = B, importance = T)

yhat = predict(rf.d, newdata = d.test, type = "class")
response.test = d.test$diabetes
d.misclass = table(yhat, response.test)
d.misclass
rf.misclass <- 1 - sum(diag(d.misclass))/sum(d.misclass)

```
The resulting test misclassification rate is `r rf.misclass`. This is slightly lower than for the simple tree. Next, we find the most influential variables in this classification problem.
```{r}
# Find the most important variables.
importance(rf.d)
varImpPlot(rf.d)
```
We see that `glu` and `bmi` are the most influential.

# Problem 4

## a)

TRUE, Perhaps (usikker her, \textcolor{green}{Ja, det skal vel være lett å finne en separating hyperplane siden p >> n, så kanksje FALSE er riktig?}), FALSE (usikker, \textcolor{green}{logreg bryter jo sammen når p >n, men det finnes vel ridge-metoder for logreg som gjør at det funker?}), TRUE

## b)

```{r 4b}
 id <- "1x_E8xnmz9CMHh_tMwIsWP94czPa1Fpsj"  # google file ID
 d.leukemia <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)
set.seed(2399)
t.samples <- sample(1:60, 15, replace = F)
d.leukemia$Category <- as.factor(d.leukemia$Category)
d.leukemia.test <- d.leukemia[t.samples, ]
d.leukemia.train <- d.leukemia[-t.samples, ]
```

(i) A support vector machine (SVM) is more suitable than a logistic regression here, because $p > n$. We could also use linear and quadratic discriminant analysis, in addition to logistic regression variations with a penalty term (similar to e.g. ridge regression).

(ii) The paper intends to demonstrate a new method for identifying the subset of genes (predictors) which capture the necessary information in order to classify patients into different groups (response). They introduce the Ensemble SVM-Recursive Feature Elimination (ESVM-RFE), which combines the ensamble and bagging methods used in random forests with SVMs and a bakcward elimination strategy.

(iii)
```{r}
svcfit <-  svm(Category~., data = d.leukemia.train, kernel = "linear", cost = 1, scale = T)

train.yhat <- predict(svcfit, newdata = d.leukemia.train)
train.misclass <- table(predict = train.yhat, truth = d.leukemia.train$Category)
train.misclass
1 - sum(diag(train.misclass)) / sum(train.misclass)

test.yhat <- predict(svcfit, newdata = d.leukemia.test) 
test.misclass <- table(predict = test.yhat, truth = d.leukemia.test$Category)
test.misclass
```

The misclassification error rates are `r 1 - sum(diag(train.misclass)) / sum(train.misclass)` for the training data and `r 1 - sum(diag(test.misclass)) / sum(test.misclass)` for the test data.

The training error rate is not surprising, as finding a spearaitng hyperplane when $p >> n$ is easy. The support vector classifier has 3 false positives and 3 false negatives on the test set. \textcolor{red}{kanskje det ikke er riktig?}

(iv)

## c)
$$
\begin{split}
K(\boldsymbol{x}_i, \boldsymbol{x}_i') &= (1 + \sum_{j=1}^2x_{ij}x_{ij}')^2 \\ &= (x_{i1}x_{i1}' + x_{i2}x_{i2}' + 1)^2 \\
&= (x_{i1}x_{i1}')^2 + (x_{i2}x_{i2}')^2 + 2x_{i1}x_{i2}\cdot x_{i1}'x_{i2}' + 2x_{i1}x_{i1}' + 2x_{i2}x_{i2}' + 1 \\
&= \langle h(\boldsymbol{x}_i), h(\boldsymbol{x}_i')\rangle,
\end{split}
$$
where
$$
h(\boldsymbol{x}_i) = (x_{i1}^2, x_{i2}^2,\sqrt{2}x_{i1}x_{i2}, \sqrt{2}x_{i1}, \sqrt{2}x_{i2}, 1)^T.
$$
# Problem 5

## a)

TRUE, FALSE, FALSE\textcolor{red}{hva menes med robust da?} \textcolor{green}{Alex: Her tenker jeg at de lurer på om resultatet K-means clustering avhenger sterkt av den initielle randomiseringen av punktene i ulike clusters. Jeg tenker at dette er FALSE, nettopp fordi algoritmen kjøres for mange ulike randomiseringer, før den beste velges. Dette viser jo at den avhenger av det tilfeldige valget i starten, og er dermed ikke robust mot dette valget. Vi kan også spørre om dette er rett tolkning eventuelt ;)}, FALSE

## b)

(i)
```{r}
set.seed(1)
x1 <- c(1, 2, 0, 4, 5, 6)
x2 <- c(5, 4, 3, 1, 1, 2)

K = 2
cluster <-  sample(1:K, length(x1), replace = T)
df = as.data.frame(cbind(x1, x2, cluster))
df$cluster = as.factor(df$cluster)

plt <- ggplot(data = df, aes(x1, x2, color = cluster) ) +
        geom_point() +
        theme_bw()
plt
```

(ii) \textcolor{red}{mulig at dette kan gjøres på en bedre måte, for nå endrer fargene seg og det er ikke tydelig hvilken cluster sentroidene tilhører i plottet.}
```{r}
calc_centr <- function(k, df) {
  feats <- df[df$cluster == k, 1:2]
  mean.1 <- mean(feats[, 1])
  mean.2 <- mean(feats[, 2])
  return(c(mean.1, mean.2))
}

c1 <- calc_centr(1, df)
c2 <- calc_centr(2, df)
centroids <- as.data.frame(rbind(c1,c2))

plt + geom_point(data = centroids, aes(V1,V2, color = "centroid")) 
```

(iii)
```{r}
eucl.dist <- function(x, y) {
  d <- length(x) # dimension
  sum = 0
  x
  y
  for(i in 1:d) {
    sum = sum + (x[i] - y[i])^2
  }
  return(sqrt(sum))
}

reassign <- function(df, centroids){
  n = nrow(df)
  for(i in 1:n) {
    dist1 <-  eucl.dist(df[i, 1:2], centroids[1, ])
    dist2 <- eucl.dist(df[i, 1:2], centroids[2, ])
    if(dist1 > dist2) {
    df[i, 3]  <- 2
    } else {
      df[i, 3] <- 1
    }
  }
  return(df)
}

df <- reassign(df, centroids)
ggplot(data = df, aes(x1, x2, color = cluster) ) + 
  geom_point() +
  theme_bw()
```

## c)

```{r}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
                             id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData = t(GeneData)
GeneData <- scale(GeneData)
n = ncol(GeneData)
GeneData[,n]

hc.euc.complete <- hclust(dist(GeneData), method = "complete")
hc.euc.single <- hclust(dist(GeneData), method = "single")
hc.euc.average <- hclust(dist(GeneData), method = "average")
par(mfrow = c(1, 3))
plot(hc.euc.complete, main = "Complete Linkage", xlab = " ", sub = " ", cex = 0.5)
plot(hc.euc.single, main = "Single Linkage", xlab = " ", sub = " ", cex = 0.5)
plot(hc.euc.average, main = "Average Linkage", xlab = " ", sub = " ", cex = 0.5)

# Skjønner ikke helt hvorfor Gendata skal transponeres..
dd <- as.dist(1 - cor(t(GeneData)))
hc.cor.complete <- hclust(dd, method = "complete")
hc.cor.single <- hclust(dd, method = "single")
hc.cor.average <- hclust(dd, method = "average")
par(mfrow = c(1, 3))
plot(hc.cor.complete, cex.main = 0.7, main = "CL with Correlation-based Distance", xlab = " ", sub = " ", cex = 0.5)
plot(hc.cor.single, cex.main = 0.7, main = "SL with Correlation-based Distance", xlab = " ", sub = " ", cex = 0.5)
plot(hc.cor.average, cex.main = 0.7, main = "AL with Correlation-based Distance", xlab = " ", sub = " ", cex = 0.5)

```

## d)

```{r}
cutree(hc.euc.complete, 2)
cutree(hc.euc.single, 2)
cutree(hc.euc.average, 2)

cutree(hc.cor.complete, 2)
cutree(hc.cor.single, 2)
cutree(hc.cor.average, 2)
```
 \textcolor{red}{Every method classifies everything correctly!!?}

## e)

(i) \textcolor{red}{JEg tror ikke vi skal bruke biplot slik som jeg har gjort her, det blit jo bare ei suppe}
```{r}
pr.out <- prcomp(GeneData, scale = T)
biplot(pr.out, scale = 0, cex = 0.4)
```

(ii)
```{r}
pr.var <-  pr.out$sdev^2
prop <- sum(pr.var[1:5]) / sum(pr.var)
```
The proportion of variance explained by the first 5 PCs is `r round(100 * prop, 2)`%.`

## f)


