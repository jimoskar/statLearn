---
title: "Recommended Exercises (Module 2)"
author: "Jim Totland"
date: "1/16/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[Link to problem set](https://github.com/stefaniemuff/statlearning/blob/master/Exercise2/RecEx2.pdf)

## Problem 1
a) Weather forecasting. Response: "Sunny", "Cloudy", "Rain" etc. Predictors: Air pressure, temperature, and the weather of the previous day(s). The goal is to predict.

b) Battery life of a phone. Response: Time until the phone is dead. Predictors: Screen size, Battery specs, Processor etc. Both prediction and inference are relevant here. Given a phone, we want to be able to predict what the battery life will be, based to the predictors, but from the regression we will also be able to infer which predictors are most significant.

## Problem 2

a) In this example, the more flexible methods have a smaller test MSE. But at some point the test MSE start to increase monotonically with the flexibility. This is a result of overfitting.

b) The variance refers to how much $\hat{f}$ would change if we used another set of training data. A small variance could indicate that a rigid method has been used, implying that the data is most likely underfiited.

c) Bias generally decreases with flexibility, which indicates that a very low bias is connected to overfitting the data. 


## Problem 3

```{r chunk-name, results = "hide"}
library(ISLR)
data(Auto)
```
```{r, include = F}
library(tidyverse)
```


a) Use the `glimpse` function from the tidyverse:

```{r}
glimpse(Auto)
```
The data has dimensions $392 \times 9$. All predictors except `name` are quantitative, although some of the them may also be treated as categorical.

b)
The range is found by applying the `range()` function. For example:
```{r}
range(Auto$mpg)
```

c) The mean and standard deviation can be found in the following way:
```{r}
for (i in 1:8) {
  print(summarise(Auto, mean = mean(Auto[,i]), sd = sd(Auto[,i])))
}
```

d) Possible, though not very clean, solution:

```{r}
ReducedAuto <- Auto[- (10:85),]

for (i in 1:8) {
  print(summarise(ReducedAuto, mean = mean(ReducedAuto[,i]), 
                  sd = sd(ReducedAuto[,i]), 
                  range = range(ReducedAuto[,i])))
}
```

e)
```{r, cache = T}
library(GGally)
ggpairs(Auto[-9])
```

From the plot we can see that there seems to be a linear relationship between multiple predictors. E.g. `weight` and `displacement` have a clearly positive linear relationship. There also seems to be some non-linear relationships, e.g. between `mpg` and `horsepower`.

f) I will here treat `cylinders` and `origin` as qualitative variables and get the following box plots:

```{r, echo = F, cache = T}
Auto$origin = as.factor(Auto$origin)
ggplot(Auto) + 
  geom_boxplot(mapping = aes(x = origin, y = mpg))

Auto$cylinders = as.factor(Auto$cylinders)
ggplot(Auto) + 
  geom_boxplot(mapping = aes(x = cylinders, y = mpg))
```

The majority of the variables seem to have some relvance in predicrting `mpg`. But the variables `year`, `acceleration` and `name` are probably the least impactful based on visual inspection.

g) The following function calculates the correlation matrix given the covariance matrix.

```{r}
getCor <- function(covMat) {
  rows <- dim(covMat)[1]
  cols <- dim(covMat)[2]
  corMat <- matrix(nrow = rows, ncol = cols)
  
  for (i in 1:rows) {
    for(j in 1:cols) {
      corMat[i,j] = covMat[i,j] / (sqrt(covMat[i,i]) * sqrt(covMat[j,j]))
    }
  }
  return (corMat)
}
```






