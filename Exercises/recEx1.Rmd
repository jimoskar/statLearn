---
title: "Recommended Exercises (Module 2)"
author: "Jim Totland"
date: "1/16/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[Link to problem set](https://github.com/stefaniemuff/statlearning/blob/master/Exercise2/RecEx2.pdf)

## Problem 1
a) Weather forecasting. Response: "Sunny", "Cloudy", "Rain" etc. Predictors: Air pressure, temperature, and the weather of the previous day(s). The goal is to predict.

b) Battery life of a phone. Response: Time until the phone is dead. Predictors: Screen size, Battery specs, Processor etc. Both prediction and inference are relevant here. Given a phone, we want to be able to predict what the battery life will be, based to the predictors, but from the regression we will also be able to infer which predictors are most significant.

## Problem 2

a) In this example, the more flexible methods have a smaller test MSE. But at some point the test MSE start to increase monotonically with the flexibility. This is a result of overfitting.

b) The variance refers to how much $\hat{f}$ would change if we used another set of training data. A small variance could indicate that a rigid method has been used, implying that the data is most likely underfitted.

c) Bias generally decreases with flexibility, which indicates that a very low bias is connected to overfitting the data. 


## Problem 3

```{r chunk-name, results = "hide"}
library(ISLR)
data(Auto)
library(tidyverse)
```


a) Use the `glimpse` function from the tidyverse:

```{r}
glimpse(Auto)
```
The data has dimensions $392 \times 9$. All predictors except `name` are quantitative, although some of the them may also be treated as categorical.

b)
The range is found by applying the `range()` function. For example:
```{r}
range(Auto$mpg)
```
Alternatively use `sapply`:
```{r}
quant = c(1,3,4,5,6,7)
sapply(Auto[, quant], range)
```

c) The mean and standard deviation can be found in the following way:
```{r}
sapply(Auto[, quant], mean)
sapply(Auto[, quant], sd)
```

d) Possible, though not very clean, solution:

```{r}
ReducedAuto <- Auto[- (10:85),]
# range, mean and sd
sapply(ReducedAuto[, quant], range)
sapply(ReducedAuto[, quant], mean)
sapply(ReducedAuto[, quant], sd)

```

e)
```{r, cache = T}
library(GGally)
ggpairs(Auto[, quant])
```

From the plot we can see that there seems to be a linear relationship between multiple predictors. E.g. `weight` and `displacement` have a clearly positive linear relationship. There also seems to be some non-linear relationships, e.g. between `mpg` and `horsepower`.

f) I will here treat `cylinders` and `origin` as qualitative variables and get the following box plots:

```{r, echo = F, cache = T}
Auto$origin = as.factor(Auto$origin)
ggplot(Auto) + 
  geom_boxplot(mapping = aes(x = origin, y = mpg))

Auto$cylinders = as.factor(Auto$cylinders)
ggplot(Auto) + 
  geom_boxplot(mapping = aes(x = cylinders, y = mpg))
```

The majority of the variables seem to have some relevance in predicting `mpg`. But the variables `year`, `acceleration` and `name` are probably the least important based on visual inspection.

g) The following function calculates the correlation matrix given the covariance matrix.

```{r}
getCor <- function(covMat) {
  rows <- dim(covMat)[1]
  cols <- dim(covMat)[2]
  corMat <- matrix(nrow = rows, ncol = cols)
  
  for (i in 1:rows) {
    for(j in 1:cols) {
      corMat[i,j] = covMat[i,j] / (sqrt(covMat[i,i]) * sqrt(covMat[j,j]))
    }
  }
  return (corMat)
}
```

## Problem 4

a) 
```{r}
library(MASS)
v1 <- as.data.frame(mvrnorm(n = 1000, 
              mu = c(2,3), 
              Sigma = matrix(c(1, 0, 0, 1), 2, 2, byrow = T) ))
colnames(v1) = c("x1", "x2")

v2 <- as.data.frame(mvrnorm(n = 1000, 
              mu = c(2,3), 
              Sigma = matrix(c(1, 0, 0, 5), 2, 2, byrow = T) ))
colnames(v2) = c("x1", "x2")

v3 <- as.data.frame(mvrnorm(n = 1000, 
              mu = c(2,3), 
              Sigma = matrix(c(1, 2, 2, 5), 2, 2, byrow = T) ))
colnames(v3) = c("x1", "x2")

v4 <- as.data.frame(mvrnorm(n = 1000, 
              mu = c(2,3), 
              Sigma = matrix(c(1, -2, -2, 5), 2, 2, byrow = T) ) )
colnames(v4) = c("x1", "x2")
```
b) Plot the simulated distributions:
```{r}
library(gridExtra)
p1 <-  ggplot(v1, aes(x1, x2)) + geom_point() + labs(title = "set1") + theme_minimal()
p2 <-  ggplot(v2, aes(x1, x2)) + geom_point() + labs(title = "set2") + theme_minimal()
p3 <-  ggplot(v3, aes(x1, x2)) + geom_point() + labs(title = "set3") + theme_minimal()
p4 <-  ggplot(v4, aes(x1, x2)) + geom_point() + labs(title = "set4") + theme_minimal()
grid.arrange(p1,p2,p3,p4, ncol = 2)
```

## Problem 5

a) Supplied code:

```{r, cache = T}
library(ggplot2)
library(ggpubr)
set.seed(2)  # to reproduce
M = 100  # repeated samplings, x fixed 
nord = 20  # order of polynoms
x = seq(from = -2, to = 4, by = 0.1)
truefunc = function(x) {
    return(x^2)
}
true_y = truefunc(x)
error = matrix(rnorm(length(x) * M, mean = 0, sd = 2), nrow = M, byrow = TRUE)
ymat = matrix(rep(true_y, M), byrow = T, nrow = M) + error
predarray = array(NA, dim = c(M, length(x), nord))
for (i in 1:M) {
    for (j in 1:nord) {
        predarray[i, , j] = predict(lm(ymat[i, ] ~ poly(x, j, raw = TRUE)))
    }
}
# M matrices of size length(x) times nord first, only look at
# variablity in the M fits and plot M curves where we had 1 for
# plotting need to stack the matrices underneath eachother and make
# new variable 'rep'
stackmat = NULL
for (i in 1:M) {
    stackmat = rbind(stackmat, cbind(x, rep(i, length(x)), predarray[i, 
        , ]))
}
# dim(stackmat)
colnames(stackmat) = c("x", "rep", paste("poly", 1:20, sep = ""))
sdf = as.data.frame(stackmat)  #NB have poly1-20 now - but first only use 1,2,20
# to add true curve using stat_function - easiest solution
true_x = x
yrange = range(apply(sdf, 2, range)[, 3:22])
p1 = ggplot(data = sdf, aes(x = x, y = poly1, group = rep, colour = rep)) + 
    scale_y_continuous(limits = yrange) + geom_line()
p1 = p1 + stat_function(fun = truefunc, lwd = 1.3, colour = "black") + 
    ggtitle("poly1")
p2 = ggplot(data = sdf, aes(x = x, y = poly2, group = rep, colour = rep)) + 
    scale_y_continuous(limits = yrange) + geom_line()
p2 = p2 + stat_function(fun = truefunc, lwd = 1.3, colour = "black") + 
    ggtitle("poly2")
p10 = ggplot(data = sdf, aes(x = x, y = poly10, group = rep, colour = rep)) + 
    scale_y_continuous(limits = yrange) + geom_line()
p10 = p10 + stat_function(fun = truefunc, lwd = 1.3, colour = "black") + 
    ggtitle("poly10")
p20 = ggplot(data = sdf, aes(x = x, y = poly20, group = rep, colour = rep)) + 
    scale_y_continuous(limits = yrange) + geom_line()
p20 = p20 + stat_function(fun = truefunc, lwd = 1.3, colour = "black") + 
    ggtitle("poly20")
ggarrange(p1, p2, p10, p20)
```

Unsuprisingly, the second degree polynomial fits best.

b) The code is found in on the exercise sheet, We get the following plots.

```{r, echo = F}
set.seed(2)  # to reproduce
M = 100  # repeated samplings,x fixed but new errors
nord = 20
x = seq(from = -2, to = 4, by = 0.1)
truefunc = function(x) {
  return(x^2)
}
true_y = truefunc(x)
error = matrix(rnorm(length(x) * M, mean = 0, sd = 2), nrow = M, byrow = TRUE)
testerror = matrix(rnorm(length(x) * M, mean = 0, sd = 2), nrow = M, 
                   byrow = TRUE)
ymat = matrix(rep(true_y, M), byrow = T, nrow = M) + error
testymat = matrix(rep(true_y, M), byrow = T, nrow = M) + testerror
predarray = array(NA, dim = c(M, length(x), nord))
for (i in 1:M) {
  for (j in 1:nord) {
    predarray[i, , j] = predict(lm(ymat[i, ] ~ poly(x, j, raw = TRUE)))
  }
}
trainMSE = matrix(ncol = nord, nrow = M)
testMSE = matrix(ncol = nord, nrow = M)
for (i in 1:M) {
  trainMSE[i, ] = apply((predarray[i, , ] - ymat[i, ])^2, 2, mean)
  testMSE[i, ] = apply((predarray[i, , ] - testymat[i, ])^2, 2, mean)
}

library(ggplot2)
library(ggpubr)
# format suitable for plotting
stackmat = NULL
for (i in 1:M) {
  stackmat = rbind(stackmat, cbind(rep(i, nord), 1:nord, trainMSE[i, 
                                                                  ], testMSE[i, ]))
}
colnames(stackmat) = c("rep", "poly", "trainMSE", "testMSE")
sdf = as.data.frame(stackmat)
yrange = range(sdf[, 3:4])
p1 = ggplot(data = sdf[1:nord, ], aes(x = poly, y = trainMSE)) + scale_y_continuous(limits = yrange) + 
  geom_line()
pall = ggplot(data = sdf, aes(x = poly, group = rep, y = trainMSE, colour = rep)) + 
  scale_y_continuous(limits = yrange) + geom_line()
testp1 = ggplot(data = sdf[1:nord, ], aes(x = poly, y = testMSE)) + scale_y_continuous(limits = yrange) +   geom_line()

testpall = ggplot(data = sdf, aes(x = poly, group = rep, y = testMSE, colour = rep)) + scale_y_continuous(limits = yrange) + geom_line()
ggarrange(p1, pall, testp1, testpall)


library(reshape2)
df = melt(sdf, id = c("poly", "rep"))[, -2]
colnames(df)[2] = "MSEtype"
ggplot(data = df, aes(x = as.factor(poly), y = value)) + geom_boxplot(aes(fill = MSEtype))

trainMSEmean = apply(trainMSE, 2, mean)
testMSEmean = apply(testMSE, 2, mean)
meandf = melt(data.frame(cbind(poly = 1:nord, trainMSEmean, testMSEmean)), 
              id = "poly")
ggplot(data = meandf, aes(x = poly, y = value, colour = variable)) + 
  geom_line()
```

We can see that the second degreee polynimial achives the lowest men testMSE, while the average trainMSE increases with polynomial degree.

c)

```{r, echo = F}
meanmat = matrix(ncol = length(x), nrow = nord)
varmat = matrix(ncol = length(x), nrow = nord)
for (j in 1:nord) {
  meanmat[j, ] = apply(predarray[, , j], 2, mean)  # we now take the mean over the M simulations - to mimic E and Var at each x value and each poly model
  varmat[j, ] = apply(predarray[, , j], 2, var)
}
# nord times length(x)
bias2mat = (meanmat - matrix(rep(true_y, nord), byrow = TRUE, nrow = nord))^2  #here the truth is finally used!

df = data.frame(rep(x, each = nord), rep(1:nord, length(x)), c(bias2mat), 
                c(varmat), rep(4, prod(dim(varmat))))  #irr is just 1
colnames(df) = c("x", "poly", "bias2", "variance", "irreducible error")  #suitable for plotting
df$total = df$bias2 + df$variance + df$`irreducible error`
hdf = melt(df, id = c("x", "poly"))
hdf1 = hdf[hdf$poly == 1, ]
hdf2 = hdf[hdf$poly == 2, ]
hdf10 = hdf[hdf$poly == 10, ]
hdf20 = hdf[hdf$poly == 20, ]
p1 = ggplot(data = hdf1, aes(x = x, y = value, colour = variable)) + 
  geom_line() + ggtitle("poly1")
p2 = ggplot(data = hdf2, aes(x = x, y = value, colour = variable)) + 
  geom_line() + ggtitle("poly2")
p10 = ggplot(data = hdf10, aes(x = x, y = value, colour = variable)) + 
  geom_line() + ggtitle("poly10")
p20 = ggplot(data = hdf20, aes(x = x, y = value, colour = variable)) + 
  geom_line() + ggtitle("poly20")
ggarrange(p1, p2, p10, p20)

hdfatxa = hdf[hdf$x == -1, ]
hdfatxb = hdf[hdf$x == 0.5, ]
hdfatxc = hdf[hdf$x == 2, ]
hdfatxd = hdf[hdf$x == 3.5, ]
pa = ggplot(data = hdfatxa, aes(x = poly, y = value, colour = variable)) + 
  geom_line() + ggtitle("x0=-1")
pb = ggplot(data = hdfatxb, aes(x = poly, y = value, colour = variable)) + 
  geom_line() + ggtitle("x0=0.5")
pc = ggplot(data = hdfatxc, aes(x = poly, y = value, colour = variable)) + 
  geom_line() + ggtitle("x0=2")
pd = ggplot(data = hdfatxd, aes(x = poly, y = value, colour = variable)) + 
  geom_line() + ggtitle("x0=3.5")
ggarrange(pa, pb, pc, pd)
```

d)



